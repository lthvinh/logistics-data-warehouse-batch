# name: logistics

x-airflow-common:
  &airflow-common
  
  # image: apache/airflow:2.10.4

  image: custom-airflow
  build:
    context: ./airflow
    dockerfile: Dockerfile

  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://airflow:airflow@mysql/airflow?charset=utf8mb4
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    DBT_PROFILES_DIR: /opt/dbt/.dbt

  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${SOURCE_DIR:-.}:/opt/src
    - ${DBT_DIR:-.}:/opt/dbt

  user: "${AIRFLOW_UID:-50000}:0"
  networks:
    - app-tier
    
  depends_on:
    &airflow-common-depends-on
    mysql:
      condition: service_healthy

services:
  mysql:
    image: mysql:8
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: Vinh@123456
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    networks:
      - app-tier
    ports:
      - "3306:3306"
    volumes:
      - ./mysql-storage:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  clickhouse:
    image: clickhouse/clickhouse-server:latest-alpine
    container_name: clickhouse
    networks:
      - app-tier
    ports:
      - 8123:8123
      - 9000:9000
      - 9009:9009
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
    environment:
      - CLICKHOUSE_DB=logistics
      - CLICKHOUSE_USER=vinh
      - CLICKHOUSE_PASSWORD=Vinh@123456

    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    container_name: "airflow-webserver"
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    container_name: "airflow-scheduler"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    container_name: "airflow-init"
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  hdfs-namenode:
    image: gchq/hdfs:${HADOOP_VERSION}
    healthcheck:
      test: curl -f http://localhost:9870 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
    build:
      context: ./hadoop
      args:
        HADOOP_VERSION: ${HADOOP_VERSION}

    command: namenode
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    environment:
    - HADOOP_VERSION=3.3.3
    - HADOOP_CONF_DIR=/etc/hadoop/conf
    networks:
      - app-tier
    ports:
    - 9870:9870
    volumes:
    - ./hadoop/conf:${HADOOP_CONF_DIR}:ro
    - /var/log/hadoop
    - /data1
    - /data2

  hdfs-datanode:
    image: gchq/hdfs:${HADOOP_VERSION}
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    command: datanode
    container_name: hdfs-datanode
    hostname: hdfs-datanode
    networks:
      - app-tier
    environment:
    - HADOOP_VERSION=3.3.3
    - HADOOP_CONF_DIR=/etc/hadoop/conf
    volumes:
    - ./hadoop/conf:${HADOOP_CONF_DIR}:ro
    - /var/log/hadoop
    - /data1
    - /data2

  pyspark:
    image: "quay.io/jupyter/pyspark-notebook:spark-3.5.3"
    container_name: pyspark
    volumes:
      - ./airflow/dags:/home/jovyan/work/dags
      - ./src:/home/jovyan/work/src
    networks:
      - app-tier
    ports:
      - 8888:8888
      - 4040:4040
  livy:
    image: livy:latest
    build:
      context: ./livy
      dockerfile: Dockerfile
    container_name: livy
    command: ['/opt/livy/bin/livy-server']
    volumes:
      - ./livy/conf:/opt/livy/conf
      - ./src:/src
    networks:
      - app-tier
    ports:
      - 8998:8998

  spark-master:
    image: custom-spark:latest
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    volumes:
      - ./src:/opt/spark-apps
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - app-tier

  spark-worker:
    image: custom-spark:latest
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./src:/opt/spark-apps
    depends_on:
      - spark-master
    networks:
      - app-tier

  kafka:
    image: 'bitnami/kafka:latest'
    container_name: kafka
    networks:
      - app-tier
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER

  kafka-schema-registry:
    image: bitnami/schema-registry:latest
    container_name: kafka-schema-registry
    depends_on:
      - kafka
    networks:
      - app-tier
    ports:
      - "8081:8081"
    environment:
      - SCHEMA_REGISTRY_KAFKA_BROKERS=PLAINTEXT://kafka:9092
      - SCHEMA_REGISTRY_HOST_NAME=kafka-schema-registry
      - SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081

  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    container_name: kafka-connect
    depends_on:
      - kafka
    networks:
      - app-tier
    ports:
      - "8083:8083"
    volumes:
      - ./kafka-connect/connect_plugins/clickhouse-clickhouse-kafka-connect-v1.2.6/:/usr/share/java/clickhouse-clickhouse-kafka-connect-v1.2.6
      - ./kafka-connect/connect_plugins/confluentinc-kafka-connect-hdfs3-1.2.2/:/usr/share/java/confluentinc-kafka-connect-hdfs3-1.2.2
      - ./kafka-connect/connect_plugins/debezium-connector-mysql:/usr/share/java/debezium-connector-mysql
    environment:
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect
      - CONNECT_REST_PORT=8083
      - CONNECT_GROUP_ID=compose-connect-group
      - CONNECT_CONFIG_STORAGE_TOPIC=docker-connect-configs
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_OFFSET_STORAGE_TOPIC=docker-connect-offsets
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_STATUS_STORAGE_TOPIC=docker-connect-status
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_KEY_CONVERTER=io.confluent.connect.avro.AvroConverter
      - CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=http://kafka-schema-registry:8081
      - CONNECT_VALUE_CONVERTER=io.confluent.connect.avro.AvroConverter
      - CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=http://kafka-schema-registry:8081
      - CONNECT_PLUGIN_PATH=/usr/share/java
  
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    networks:
      - app-tier
    ports:
      - "9090:8080"
    depends_on:
      - kafka
      - kafka-connect
      - kafka-schema-registry
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME=connect
      - KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS=http://kafka-connect:8083
      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://kafka-schema-registry:8081
      - DYNAMIC_CONFIG_ENABLED=true
      
networks:
  app-tier:
    external: true

volumes:
  clickhouse-data:
    driver: local

  clickhouse-logs:
    driver: local