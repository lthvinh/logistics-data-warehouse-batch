{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8b86d5-fb98-44d5-819b-34c80886b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import yaml\n",
    "from typing import Optional\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f11437-55f5-4842-95ea-236de13cdb4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Config' from 'config' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecorator_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoratorFactory\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Config' from 'config' (unknown location)"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "from decorator_factory import DecoratorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb95018b-bbf1-464c-9c99-3505f07013e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecoratorFactory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mextract\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mraw_data_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/work/logistics_project_v2/extract/raw_data_factory.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mRawDataFactory\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n",
      "File \u001b[0;32m~/work/logistics_project_v2/extract/raw_data_factory.py:18\u001b[0m, in \u001b[0;36mRawDataFactory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_month \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_date_details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_date_details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;129m@DecoratorFactory\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_get_raw_data\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_raw_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m, raw_data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     20\u001b[0m     raw_df_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_data_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDFS Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecoratorFactory' is not defined"
     ]
    }
   ],
   "source": [
    "from extract.raw_data_factory import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d8d6e-901f-47e9-9490-6bbb729813bf",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36f720d-e1de-4de0-b16c-3da783c05f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import pendulum\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "from delta import *\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        self.config_path = config_path\n",
    "        \n",
    "        self.current_date_details = {\n",
    "            'date': pendulum.now(tz='Asia/Ho_Chi_Minh')\n",
    "            , 'year': pendulum.now(tz='Asia/Ho_Chi_Minh').year\n",
    "            , 'month': pendulum.now(tz='Asia/Ho_Chi_Minh').month\n",
    "            , 'day': pendulum.now(tz='Asia/Ho_Chi_Minh').day\n",
    "        }\n",
    "\n",
    "        # self.one_day_delta = pendulum.timedelta(days=1)\n",
    "        self.one_day_delta = 10\n",
    "        \n",
    "        self.previous_date_details = {\n",
    "            'date': pendulum.now(tz='Asia/Ho_Chi_Minh').subtract(days = self.one_day_delta)\n",
    "            , 'year': pendulum.now(tz='Asia/Ho_Chi_Minh').subtract(days = self.one_day_delta).year\n",
    "            , 'month': pendulum.now(tz='Asia/Ho_Chi_Minh').subtract(days = self.one_day_delta).month\n",
    "            , 'day': pendulum.now(tz='Asia/Ho_Chi_Minh').subtract(days = self.one_day_delta).day\n",
    "        }\n",
    "\n",
    "        self.configs = self._get_configs()\n",
    "\n",
    "        self.data_lake_configs = self.configs.get('data_lake')\n",
    "        self.raw_configs = self.data_lake_configs['raw']\n",
    "        self.enriched_configs = self.data_lake_configs['enriched']\n",
    "        self.spark_configs = self.configs['spark']\n",
    "        self.logging_configs = self.configs['logging']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def spark(self):\n",
    "        \n",
    "        spark_config = self.spark_configs.get('config', {})\n",
    "        spark_packages = self.spark_configs.get('packages')\n",
    "        spark_app_name = self.spark_configs.get('app_name', 'temp_name')\n",
    "        \n",
    "        builder = (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .config(map = spark_config)\n",
    "            .appName(spark_app_name)\n",
    "        )\n",
    "        spark = configure_spark_with_delta_pip(\n",
    "            spark_session_builder = builder\n",
    "            , extra_packages = spark_packages\n",
    "        ).getOrCreate()\n",
    "        \n",
    "        return spark\n",
    "        \n",
    "    @property\n",
    "    def logger(self):\n",
    "        \n",
    "        logging_level = self.logging_configs.get('level', 'INFO')\n",
    "        logging_format = self.logging_configs.get('format')\n",
    "        logging_base_path = self.logging_configs.get('base_path')\n",
    "\n",
    "        current_year = self.current_date_details['year']\n",
    "        current_month = self.current_date_details['month']\n",
    "        current_day = self.current_date_details['day']\n",
    "        \n",
    "        logging_path = f'{logging_base_path}/{current_year}/{current_month:0>2}'\n",
    "        logging_file_name = f'{current_year}{current_month:0>2}{current_day:0>2}.log'\n",
    "        \n",
    "        logging_path = Path.cwd() / Path(logging_path)\n",
    "        logging_path.mkdir(exist_ok=True, parents=True)\n",
    "        logging_file_path = logging_path / logging_file_name\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level = logging_level\n",
    "            , format = logging_format\n",
    "            , handlers = [\n",
    "                logging.FileHandler(logging_file_path)\n",
    "                , logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        return logging.getLogger(__name__)\n",
    "        \n",
    "    def _get_configs(self) -> dict:\n",
    "        try:\n",
    "            if not self.config_path:\n",
    "                self.config_path = '/home/jovyan/work/logistics_project_v2/config/config.yaml'\n",
    "            with open(self.config_path) as file:\n",
    "                config = yaml.safe_load(file)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c56dac-1f05-4394-8884-3bc5a3e71e29",
   "metadata": {},
   "source": [
    "# Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8e2f5c-bb9e-45bd-8bc2-24705f1ad237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "class DecoratorFactory:\n",
    "    \n",
    "    @staticmethod\n",
    "    def decorate_get_raw_data(func):\n",
    "        @functools.wraps(func)\n",
    "        def new_function(self, raw_name, raw_data_name):\n",
    "            self.logger.info(f'Starting to fetch raw {raw_name} data from HDFS.')\n",
    "            self.logger.info(f\"DataFrame: {raw_name}\")\n",
    "\n",
    "            try:\n",
    "                self.logger.info(f'Executing function: {func.__name__}')\n",
    "                raw_df = func(self, raw_name, raw_data_name)\n",
    "                self.logger.info(f'Successfully fetched raw {raw_name} data.')\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error while fetching {raw_name} from raw layer.\")\n",
    "                self.logger.error(f\"DataFrame: {raw_name}\")\n",
    "                self.logger.error(f\"Error: {str(e)}\")\n",
    "                raise\n",
    "            return raw_df\n",
    "            \n",
    "        return new_function\n",
    "\n",
    "    @staticmethod\n",
    "    def decorate_select_columns(func):\n",
    "        @functools.wraps(func)\n",
    "        def new_function(self, raw_df):\n",
    "            try:\n",
    "                self.logger.info(f\"Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\")\n",
    "                new_raw_df = func(self, raw_df)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error while selecting columns.\")\n",
    "                self.logger.error(f\"Error: {str(e)}\")\n",
    "                raise\n",
    "            return new_raw_df\n",
    "        return new_function\n",
    "\n",
    "    @staticmethod\n",
    "    def decorate_get_enriched_df(func):\n",
    "        @functools.wraps(func)\n",
    "        def new_function(self):\n",
    "            try:\n",
    "                new_df = func(self)\n",
    "            except Exception as e:\n",
    "                self.logger.error(e)\n",
    "                raise\n",
    "            return new_df\n",
    "        return new_function\n",
    "        \n",
    "    @staticmethod\n",
    "    def decorate_load_enriched_data(func):\n",
    "        @functools.wraps(func)\n",
    "        def new_function(self, df, df_name):\n",
    "            self.logger.info(f\"Starting to load {df_name} to enriched layer.\")\n",
    "            self.logger.info(f\"DataFrame: {df_name}\")\n",
    "            try:\n",
    "                self.logger.info(f'Executing function: {func.__name__}')\n",
    "                func(self, df, df_name)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error while loading {df_name} to enriched layer.\")\n",
    "                self.logger.error(f\"DataFrame: {df_name}\")\n",
    "                self.logger.error(f\"Error: {str(e)}\")\n",
    "                raise\n",
    "            self.logger.info(f\"Successfully loaded {df_name} to enriched layer.\")\n",
    "        return new_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c61b4a-69d8-4065-ace0-8adfec79d7ba",
   "metadata": {},
   "source": [
    "# Raw Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ceb38ea2-4294-4877-a95b-0811b9aee057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataFactory:\n",
    "    def __init__(self, config = Config):\n",
    "        self.app_config = config\n",
    "        self.spark = self.app_config.spark\n",
    "        self.logger = self.app_config.logger\n",
    "        \n",
    "        self.raw_configs = self.app_config.raw_configs\n",
    "        self.raw_base_path = self.raw_configs.get('base_path')\n",
    "        self.raw_format =  self.raw_configs.get('format')\n",
    "        \n",
    "        self.previous_date_details = self.app_config.previous_date_details\n",
    "        self.previous_year = self.previous_date_details['year']\n",
    "        self.previous_month = self.previous_date_details['month']\n",
    "        self.previous_day = self.previous_date_details['day']\n",
    "\n",
    "    @DecoratorFactory.decorate_get_raw_data\n",
    "    def _get_raw_data(self, raw_name = str, raw_data_name = str) -> DataFrame:\n",
    "        raw_df_path = f'{self.raw_base_path}/{raw_data_name}'\n",
    "        self.logger.info(f\"HDFS Path: {raw_df_path}\")\n",
    "        raw_df = (\n",
    "            self.spark.read\n",
    "            .format(self.raw_format)\n",
    "            .option('path', raw_df_path)\n",
    "            .load()\n",
    "            .where(\n",
    "                (F.col('year') == self.previous_year)\n",
    "                & (F.col('month') == self.previous_month)\n",
    "                & (F.col('day') == self.previous_day)\n",
    "            )\n",
    "        )\n",
    "        return raw_df\n",
    "\n",
    "    @property\n",
    "    def raw_users_df(self):\n",
    "        raw_users_name = 'users'\n",
    "        raw_users_data_name = 'logistics_src.logistics.Users'\n",
    "        return self._get_raw_data(raw_users_name, raw_users_data_name)\n",
    "        \n",
    "    @property\n",
    "    def raw_drivers_df(self):\n",
    "        raw_drivers_name = 'drivers'\n",
    "        raw_drivers_data_name = 'logistics_src.logistics.Drivers'\n",
    "        return self._get_raw_data(raw_drivers_name, raw_drivers_data_name)\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def raw_orders_df(self):\n",
    "        raw_orders_name = 'orders'\n",
    "        raw_orders_data_name = 'logistics_src.logistics.Orders'\n",
    "        return self._get_raw_data(raw_orders_name, raw_orders_data_name)\n",
    "        \n",
    "    @property\n",
    "    def raw_payments_df(self):\n",
    "        raw_payments_name = 'payments'\n",
    "        raw_payments_data_name = 'logistics_src.logistics.Payments'\n",
    "        return self._get_raw_data(raw_payments_name, raw_payments_data_name)\n",
    "        \n",
    "    @property\n",
    "    def raw_shipments_df(self):\n",
    "        raw_shipments_name = 'shipments'\n",
    "        raw_shipments_data_name = 'logistics_src.logistics.Shipments'\n",
    "        return self._get_raw_data(raw_shipments_name, raw_shipments_data_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94508b-c66f-4e2f-9191-725b688d4fff",
   "metadata": {},
   "source": [
    "# Enriched Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181b399b-979a-451d-8cc2-cf076d821c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea32e65d-f406-4bb6-84f9-f067a47a9289",
   "metadata": {},
   "source": [
    "# Dim Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "39fcafcb-a992-4706-a104-cd827b098854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import Config\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class DimDate:\n",
    "    def __init__(self):\n",
    "        self.app_config = Config()\n",
    "        self.spark = self.app_config.spark\n",
    "\n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.dimensions_base_path = self.curated_configs.get('dimensions_base_path')\n",
    "        self.dim_date_path = self.dimensions_base_path + '/dim_date'\n",
    "        \n",
    "    @property\n",
    "    def dim_date(self):\n",
    "        date_range_query = \"select explode(sequence(to_date('2024-01-01'), to_date('2044-01-01'), interval 1 days)) as date\"\n",
    "        date_range_df = self.app_config.spark.sql(date_range_query)\n",
    "        dim_date = date_range_df.select(\n",
    "            F.date_format(F.col('date'), 'yyyyMMdd').cast(T.IntegerType()).alias('date_key')\n",
    "            , F.col('date')\n",
    "            , F.year(F.col('date')).alias('year')\n",
    "            , F.quarter(F.col('date')).alias('quarter')\n",
    "            , F.month(F.col('date')).alias('month')\n",
    "            , F.day(F.col('date')).alias('day')\n",
    "            , F.dayofweek(F.col('date')).alias('day_of_week')\n",
    "            , F.dayofmonth(F.col('date')).alias('day_of_month')\n",
    "            , F.dayofyear(F.col('date')).alias('day_of_year')\n",
    "            , F.weekofyear(F.col('date')).alias('week_of_year')\n",
    "            , F.when(F.dayofweek(F.col('date')).isin(1, 7), F.lit(True)).otherwise(F.lit(False)).alias('is_weekend')\n",
    "            , F.date_format(F.col('date'), 'EEEE').alias('week_name')\n",
    "            , F.date_format(F.col('date'), 'MMMM').alias('month_name')\n",
    "            , F.concat(F.lit('Q'), F.quarter(F.col('date'))).alias('quarter_name')\n",
    "        )\n",
    "        return dim_date\n",
    "        \n",
    "    def load(self):\n",
    "        is_exists = DeltaTable.isDeltaTable(self.spark, self.dim_date_path)\n",
    "        if not is_exists:\n",
    "            self.dim_date.write.mode('overwrite').format('delta').option('path', self.dim_date_path).save()\n",
    "            \n",
    "dim_date_temp = DimDate()\n",
    "dim_date_temp.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ed19e-cdf7-4222-aae0-5baeee023176",
   "metadata": {},
   "source": [
    "# Dim Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a7129fe-5883-4c3c-ad10-acf47cc44062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 09:58:13,470 - config.config - INFO - HDFS Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    }
   ],
   "source": [
    "from config.config import Config\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class DimLocations:\n",
    "    def __init__(self):\n",
    "        self.app_config = Config()\n",
    "        self.spark = self.app_config.spark\n",
    "        self.source_df = EnrichedDataFactory(self.app_config).get_source('orders')\n",
    "        \n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.dimensions_base_path = self.curated_configs.get('dimensions_base_path')\n",
    "        self.dim_locations_path = self.dimensions_base_path + '/dim_locations'\n",
    "        \n",
    "    @property\n",
    "    def dim_locations(self):\n",
    "        dim_locations = (\n",
    "            self.source_df.select(F.col('pickup_address').alias('location'))\n",
    "            .union(self.source_df.select(F.col('delivery_address').alias('location')))\n",
    "            .dropDuplicates()\n",
    "            .select(\n",
    "                F.row_number().over(Window.orderBy(F.col('location'))).alias('location_key')\n",
    "                , F.col('location')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return dim_locations\n",
    "        \n",
    "    def load(self):\n",
    "        is_exists = DeltaTable.isDeltaTable(self.spark, self.dim_locations_path)\n",
    "        if not is_exists:\n",
    "            self.dim_locations.write.mode('overwrite').format('delta').option('path', self.dim_locations_path).save()\n",
    "        else:\n",
    "            delta_table = DeltaTable.forPath(self.spark, self.dim_locations_path)\n",
    "            max_location_key = delta_table.toDF().selectExpr('max(location_key) as max_location_key').first().max_location_key\n",
    "            new_dim_locations = self.dim_locations.withColumn('location_key', F.col('location_key') + F.lit(max_location_key))\n",
    "            \n",
    "            (\n",
    "                delta_table.alias('target')\n",
    "                .merge(\n",
    "                    source = new_dim_locations.alias('source')\n",
    "                    , condition = (F.col('source.location') == F.col('target.location'))\n",
    "                )\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "            )\n",
    "            \n",
    "dim_location_temp = DimLocations()\n",
    "dim_location_temp.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a7755-71b7-4efc-bff6-62b58dca9997",
   "metadata": {},
   "source": [
    "# Dim Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "229bfc99-c474-4f6f-a09c-062f873e9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 06:40:27,822 - config.config - INFO - HDFS Path: /enriched/transactional/mysql/logistics/users\n"
     ]
    }
   ],
   "source": [
    "from config.config import Config\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class DimUsers:\n",
    "    def __init__(self):\n",
    "        self.app_config = Config()\n",
    "        self.spark = self.app_config.spark\n",
    "        self.source_df = EnrichedDataFactory(self.app_config).get_source('users')\n",
    "        \n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.dimensions_base_path = self.curated_configs.get('dimensions_base_path')\n",
    "        self.dim_users_path = self.dimensions_base_path + '/dim_users'\n",
    "        \n",
    "    @property\n",
    "    def dim_users(self):\n",
    "        \n",
    "        dim_users = self.source_df.select(\n",
    "            F.row_number().over(Window.orderBy(F.col('user_id'))).cast(T.LongType()).alias('user_key')\n",
    "            , F.col('user_id').cast(T.LongType())\n",
    "            , F.col('full_name')\n",
    "            , F.col('email')\n",
    "            , F.col('password_hash')\n",
    "            , F.col('phone_number')\n",
    "            , F.col('address')\n",
    "            , F.col('role')\n",
    "            , F.col('created_at')\n",
    "            , F.lit(True).alias('is_current')\n",
    "            , F.col('event_timestamp').alias('effective_from_timestamp')\n",
    "            , F.make_timestamp(F.lit(9999), F.lit(12), F.lit(31), F.lit(0), F.lit(0), F.lit(0)).alias('effective_to_timestamp')\n",
    "            , F.col('op')\n",
    "        )\n",
    "        return dim_users\n",
    "\n",
    "    def load(self):\n",
    "        is_exists =  DeltaTable.isDeltaTable(self.spark, self.dim_users_path)\n",
    "        if not is_exists:\n",
    "            self.dim_users.drop(F.col('op')).write.mode('overwrite').format('delta').option('path', self.dim_users_path).save()\n",
    "        else:\n",
    "            delta_table = DeltaTable.forPath(self.spark, self.dim_users_path)\n",
    "            \n",
    "            update_rows = self.dim_users.where(F.col('op') == 'u').drop(F.col('op'))\n",
    "            \n",
    "            (\n",
    "                delta_table.alias('target')\n",
    "                .merge(\n",
    "                    source = update_rows.alias('source')\n",
    "                    , condition = (F.col('source.user_id') == F.col('target.user_id'))\n",
    "                )\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = (F.col('target.is_current'))\n",
    "                    , set = {\n",
    "                        'is_current': F.lit(False)\n",
    "                        , 'effective_to_timestamp': F.col('source.effective_from_timestamp')\n",
    "                    }\n",
    "                )\n",
    "                .execute()\n",
    "            )\n",
    "            \n",
    "            max_user_key = delta_table.toDF().selectExpr('max(user_key) as max_user_key').first().max_user_key\n",
    "            new_dim_users = self.dim_users.withColumn('user_key', F.col('user_key') + F.lit(max_user_key)).drop(F.col('op'))\n",
    "            new_dim_users.write.mode('append').format('delta').option('path', self.dim_users_path).save()\n",
    "            \n",
    "users_temp = DimUsers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22f98380-dfb4-4342-ac7e-2debacf5cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_temp.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee66bef-2626-4c22-8545-a82aada708fa",
   "metadata": {},
   "source": [
    "# Dim Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b53f34be-fdfa-4c88-844f-5c4dde43454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 07:12:10,908 - config.config - INFO - HDFS Path: /enriched/transactional/mysql/logistics/drivers\n"
     ]
    }
   ],
   "source": [
    "from config.config import Config\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class DimDrivers:\n",
    "    def __init__(self):\n",
    "        self.app_config = Config()\n",
    "        self.spark = self.app_config.spark\n",
    "        self.source_df = EnrichedDataFactory(self.app_config).get_source('drivers')\n",
    "        \n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.dimensions_base_path = self.curated_configs.get('dimensions_base_path')\n",
    "        self.dim_drivers_path = self.dimensions_base_path + '/dim_drivers'\n",
    "        self.dim_users_path = self.dimensions_base_path + '/dim_users'\n",
    "        \n",
    "    @property\n",
    "    def dim_drivers(self):\n",
    "        dim_users = (\n",
    "            self.spark.read\n",
    "            .format('delta')\n",
    "            .option('path', self.dim_users_path)\n",
    "            .load()\n",
    "            .where((F.col('is_current')))\n",
    "        )\n",
    "\n",
    "        dim_drivers = (\n",
    "            self.source_df.alias('drivers')\n",
    "            .join(dim_users.alias('users'), F.col('drivers.user_id') == F.col('users.user_id'))\n",
    "            .select(\n",
    "                F.row_number().over(Window.orderBy(F.col('driver_id'))).cast(T.LongType()).alias('driver_key')\n",
    "                , F.col('driver_id').cast(T.LongType())\n",
    "                , F.col('full_name')\n",
    "                , F.col('email')\n",
    "                , F.col('password_hash')\n",
    "                , F.col('phone_number')\n",
    "                , F.col('address')\n",
    "                , F.col('vehicle_license_plate')\n",
    "                , F.col('vehicle_type')\n",
    "                , F.col('vehicle_year')\n",
    "                , F.lit(True).alias('is_current')\n",
    "                , F.col('event_timestamp').alias('effective_from_timestamp')\n",
    "                , F.make_timestamp(F.lit(9999), F.lit(12), F.lit(31), F.lit(0), F.lit(0), F.lit(0)).alias('effective_to_timestamp')\n",
    "                , F.col('op')\n",
    "            )\n",
    "        )\n",
    "        return dim_drivers\n",
    "\n",
    "    def load(self):\n",
    "        is_exists =  DeltaTable.isDeltaTable(self.spark, self.dim_drivers_path)\n",
    "        if not is_exists:\n",
    "            self.dim_drivers.drop(F.col('op')).write.mode('overwrite').format('delta').option('path', self.dim_drivers_path).save()\n",
    "        else:\n",
    "            delta_table = DeltaTable.forPath(self.spark, self.dim_drivers_path)\n",
    "            \n",
    "            update_rows = self.dim_drivers.where(F.col('op') == 'u')\n",
    "            \n",
    "            (\n",
    "                delta_table.alias('target')\n",
    "                .merge(\n",
    "                    source = update_rows.alias('source')\n",
    "                    , condition = (F.col('source.driver_id') == F.col('target.driver_id'))\n",
    "                )\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = (F.col('target.is_current'))\n",
    "                    , set = {\n",
    "                        'is_current': F.lit(False)\n",
    "                        , 'effective_to_timestamp': F.col('source.effective_from_timestamp')\n",
    "                    }\n",
    "                )\n",
    "                .execute()\n",
    "            )\n",
    "            \n",
    "            max_driver_key = delta_table.toDF().selectExpr('max(driver_key) as max_driver_key').first().max_driver_key\n",
    "            new_dim_drivers = self.withColumn('driver_key', F.col('driver_key') + F.lit(max_driver_key)).drop(F.col('op'))\n",
    "            new_dim_drivers.write.mode('append').format('delta').option('path', self.dim_drivers_path).save()\n",
    "            \n",
    "drivers_temp = DimDrivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107e612-ee2f-4053-b5a8-cad12e0305c3",
   "metadata": {},
   "source": [
    "# Fact Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98007b22-6372-4428-8ee1-8e81fc9ae4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:03:55,911 - config.config - INFO - HDFS Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------------+----------------+-------------------+--------------+-------------------+----------+-------------------+---+-------------------+-------+--------+------------+---------------+------------+---------------+\n",
      "|order_id|user_id| pickup_address|delivery_address|package_description|package_weight|      delivery_time|    status|         created_at| op|    event_timestamp|user_id|user_key|location_key|       location|location_key|       location|\n",
      "+--------+-------+---------------+----------------+-------------------+--------------+-------------------+----------+-------------------+---+-------------------+-------+--------+------------+---------------+------------+---------------+\n",
      "|     321|     40|  1 Pickup Lane| 624 Delivery Rd|          Furniture|         35.37|2024-12-31 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     40|      40|           2|  1 Pickup Lane|         617|624 Delivery Rd|\n",
      "|     322|     41|201 Pickup Lane| 823 Delivery Rd|          Furniture|         21.23|2025-01-21 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     41|      41|         131|201 Pickup Lane|         864|823 Delivery Rd|\n",
      "|     323|     41|239 Pickup Lane| 498 Delivery Rd|              Books|          0.79|2025-01-10 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     41|      41|         175|239 Pickup Lane|         475|498 Delivery Rd|\n",
      "|     324|     41|119 Pickup Lane| 406 Delivery Rd|              Books|         18.76|2025-01-04 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     41|      41|          23|119 Pickup Lane|         356|406 Delivery Rd|\n",
      "|     325|     41|559 Pickup Lane| 432 Delivery Rd|               Food|         15.23|2025-01-08 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     41|      41|         544|559 Pickup Lane|         387|432 Delivery Rd|\n",
      "|     326|     41|229 Pickup Lane| 740 Delivery Rd|               Food|         26.38|2025-01-09 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     41|      41|         166|229 Pickup Lane|         758|740 Delivery Rd|\n",
      "|     327|     42|484 Pickup Lane|  43 Delivery Rd|        Electronics|         23.32|2025-01-12 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     42|      42|         457|484 Pickup Lane|         383| 43 Delivery Rd|\n",
      "|     328|     42|410 Pickup Lane| 933 Delivery Rd|               Food|         21.25|2024-12-24 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     42|      42|         362|410 Pickup Lane|         997|933 Delivery Rd|\n",
      "|     329|     42|  1 Pickup Lane| 303 Delivery Rd|               Food|         11.89|2025-01-07 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     42|      42|           2|  1 Pickup Lane|         249|303 Delivery Rd|\n",
      "|     330|     43|969 Pickup Lane| 726 Delivery Rd|               Food|         33.78|2025-01-13 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     43|      43|        1031|969 Pickup Lane|         736|726 Delivery Rd|\n",
      "|     331|     43|479 Pickup Lane| 292 Delivery Rd|        Electronics|          27.4|2025-01-09 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     43|      43|         451|479 Pickup Lane|         236|292 Delivery Rd|\n",
      "|     332|     43|820 Pickup Lane| 159 Delivery Rd|          Furniture|         42.71|2025-01-03 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     43|      43|         861|820 Pickup Lane|          69|159 Delivery Rd|\n",
      "|     333|     44|518 Pickup Lane| 838 Delivery Rd|           Clothing|         23.95|2024-12-28 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         497|518 Pickup Lane|         879|838 Delivery Rd|\n",
      "|     334|     44|226 Pickup Lane| 688 Delivery Rd|           Clothing|          0.86|2025-01-11 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         163|226 Pickup Lane|         691|688 Delivery Rd|\n",
      "|     335|     44|366 Pickup Lane| 434 Delivery Rd|           Clothing|         32.44|2025-01-13 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         311|366 Pickup Lane|         390|434 Delivery Rd|\n",
      "|     336|     44|819 Pickup Lane| 967 Delivery Rd|              Books|         23.32|2025-01-04 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         857|819 Pickup Lane|        1029|967 Delivery Rd|\n",
      "|     337|     44|373 Pickup Lane| 712 Delivery Rd|           Clothing|          34.4|2025-01-08 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         319|373 Pickup Lane|         719|712 Delivery Rd|\n",
      "|     338|     44|981 Pickup Lane| 218 Delivery Rd|           Clothing|         16.18|2025-01-06 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|        1050|981 Pickup Lane|         151|218 Delivery Rd|\n",
      "|     339|     44|283 Pickup Lane| 615 Delivery Rd|               Food|         29.91|2024-12-24 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         227|283 Pickup Lane|         607|615 Delivery Rd|\n",
      "|     340|     44|338 Pickup Lane| 355 Delivery Rd|          Furniture|         16.09|2024-12-26 18:56:52|processing|2024-12-23 18:56:52|  r|2025-01-22 12:55:39|     44|      44|         285|338 Pickup Lane|         296|355 Delivery Rd|\n",
      "+--------+-------+---------------+----------------+-------------------+--------------+-------------------+----------+-------------------+---+-------------------+-------+--------+------------+---------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from config.config import Config\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "class FactOrders:\n",
    "    \n",
    "    def __init__(self, status = None):\n",
    "        self.app_config = Config()\n",
    "        self.spark = self.app_config.spark\n",
    "        self.source_df = EnrichedDataFactory(self.app_config).get_source('orders')\n",
    "        # where(F.col('status') == F.lit(status))\n",
    "        \n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.facts_base_path = self.curated_configs.get('facts_base_path')\n",
    "        self.fact_orders_path = self.facts_base_path + '/fact_orders'\n",
    "\n",
    "        self.dimensions_base_path = self.curated_configs.get('dimensions_base_path')\n",
    "        self.dim_users_path = self.dimensions_base_path + '/dim_users'\n",
    "        self.dim_locations_path = self.dimensions_base_path + '/dim_locations'\n",
    "\n",
    "    @property\n",
    "    def dim_users(self):\n",
    "        dim_users = (\n",
    "            self.spark.read.format('delta').option('path', self.dim_users_path).load()\n",
    "            .where((F.col('is_current')))\n",
    "            .select(F.col('user_id'), F.col('user_key'))\n",
    "        )\n",
    "        return dim_users\n",
    "        \n",
    "    @property\n",
    "    def dim_locations(self):\n",
    "        dim_locations = self.spark.read.format('delta').option('path', self.dim_locations_path).load()\n",
    "        return dim_locations\n",
    "        \n",
    "    @property\n",
    "    def joined_orders(self):\n",
    "        joined_orders = (\n",
    "            self.source_df.alias('orders')\n",
    "            .join(self.dim_users.alias('dim_users'), F.col('orders.user_id') == F.col('dim_users.user_id'), 'left')\n",
    "            .join(self.dim_locations.alias('pickup_location'), F.col('orders.pickup_address') == F.col('pickup_location.location'), 'left')\n",
    "            .join(self.dim_locations.alias('delivery_location'), F.col('orders.delivery_address') == F.col('delivery_location.location'), 'left')\n",
    "        )\n",
    "        return joined_orders\n",
    "        \n",
    "    def make_interval(self, date_key, time_key, end_timestamp):\n",
    "\n",
    "        interval = end_timestamp - F.to_timestamp(F.concat(date_key, ' ', time_key), 'yyyyMMdd HH:mm:ss')\n",
    "\n",
    "        interval_str = F.make_interval(\n",
    "            days = F.extract(F.lit('D'), interval)\n",
    "            , hours = F.extract(F.lit('H'), interval)\n",
    "            , mins = F.extract(F.lit('m'), interval)\n",
    "            , secs = F.extract(F.lit('s'), interval)\n",
    "        ).cast(T.StringType())\n",
    "        \n",
    "        return interval_str\n",
    "        \n",
    "    def write_processing_orders(self):\n",
    "\n",
    "        processing_orders_df = (\n",
    "            self.joined_orders\n",
    "            .where(F.col('status') == 'processing')\n",
    "            .select(\n",
    "                F.col('order_id')\n",
    "                , F.col('user_key')\n",
    "                , F.col('pickup_location.location_key').alias('pick_up_location_key')\n",
    "                , F.col('delivery_location.location_key').alias('delivery_location_key')\n",
    "                , F.col('package_description')\n",
    "                , F.date_format(F.col('created_at'), 'yyyyMMdd').cast(T.LongType()).alias('created_order_date_key')\n",
    "                , F.date_format(F.col('created_at'), 'HH:mm:ss').alias('created_order_time_key')\n",
    "                , F.lit(99991231).alias('accepted_date_key')\n",
    "                , F.lit('00:00:00').alias('accepted_time_key')\n",
    "                , F.lit(99991231).alias('in_transit_date_key')\n",
    "                , F.lit('00:00:00').alias('in_transit_time_key')\n",
    "                , F.lit(99991231).alias('delivered_date_key')\n",
    "                , F.lit('00:00:00').alias('delivered_time_key')\n",
    "                , F.date_format(F.col('delivery_time'), 'yyyyMMdd').cast(T.LongType()).alias('delivery_date_key')\n",
    "                , F.date_format(F.col('delivery_time'), 'HH:mm:ss').alias('delivery_time_key')\n",
    "                , F.col('package_weight')\n",
    "                , F.make_interval(days = F.lit(0), hours = F.lit(0), mins = F.lit(0), secs = F.lit(0)).cast(T.StringType()).alias('created_to_accepted_lag')\n",
    "                , F.make_interval(days = F.lit(0), hours = F.lit(0), mins = F.lit(0), secs = F.lit(0)).cast(T.StringType()).alias('accepted_to_in_transit_lag')\n",
    "                , F.make_interval(days = F.lit(0), hours = F.lit(0), mins = F.lit(0), secs = F.lit(0)).cast(T.StringType()).alias('in_transit_to_delivered_lag')\n",
    "                , F.make_interval(days = F.lit(0), hours = F.lit(0), mins = F.lit(0), secs = F.lit(0)).cast(T.StringType()).alias('delivered_and_delivery_difference')\n",
    "                , F.col('status')\n",
    "            )\n",
    "        )\n",
    "        writer = processing_orders_df.write.format('delta').option('path', self.fact_orders_path)\n",
    "        \n",
    "        is_exists = DeltaTable.isDeltaTable(self.spark, self.fact_orders_path)\n",
    "        \n",
    "        if not exists:\n",
    "            writer = writer.mode('overwrite')\n",
    "        else:\n",
    "            writer = writer.mode('append')\n",
    "            \n",
    "        writer.save()\n",
    "\n",
    "    def write_accepted_orders(self):\n",
    "\n",
    "        delta_table = DeltaTable.forPath(self.spark, self.fact_orders_path)\n",
    "        accepted_orders_df = self.joined_orders.where(F.col('status') == 'accepted')\n",
    "\n",
    "        (\n",
    "            delta_table.alias('target')\n",
    "            .merge(\n",
    "                source = accepted_orders_df.alias('source')\n",
    "                , condition = (F.col('source.order_id') == F.col('target.order_id'))\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                set = {\n",
    "                    'user_key': F.col('source.user_key')\n",
    "                    , 'pick_up_location_key': F.col('source.pick_up_location_key')\n",
    "                    , 'delivery_location_key': F.col('source.delivery_location_key')\n",
    "                    , 'package_description': F.col('source.package_description')\n",
    "                    , 'accepted_date_key': F.date_format(F.col('source.event_timestamp'), 'yyyyMMdd').cast(T.LongType())\n",
    "                    , 'accepted_time_key': F.date_format(F.col('source.event_timestamp'), 'HH:mm:ss')\n",
    "                    , 'created_to_accepted_lag': self.make_interval(F.col('target.created_order_date_key'), F.col('target.accepted_time_key'), F.col('source.event_timestamp'))\n",
    "                    , 'package_weight': F.col('source.package_weight')\n",
    "                    , 'status': F.col('source.status')\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    def write_in_transit_orders(self):\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, self.fact_orders_path)\n",
    "        in_transit_orders_df = self.joined_orders.where(F.col('status') == 'in_transit')\n",
    "        \n",
    "        (\n",
    "            delta_table.alias('target')\n",
    "            .merge(\n",
    "                source = in_transit_orders_df.alias('source')\n",
    "                , condition = (F.col('source.order_id') == F.col('target.order_id'))\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                set = {\n",
    "                    'user_key': F.col('source.user_key')\n",
    "                    , 'pick_up_location_key': F.col('source.pick_up_location_key')\n",
    "                    , 'delivery_location_key': F.col('source.delivery_location_key')\n",
    "                    , 'package_description': F.col('source.package_description')\n",
    "                    , 'in_transit_date_key': F.date_format(F.col('source.event_timestamp'), 'yyyyMMdd').cast(T.LongType())\n",
    "                    , 'in_transit_time_key': F.date_format(F.col('source.event_timestamp'), 'HH:mm:ss')\n",
    "                    , 'accepted_to_in_transit_lag': self.make_interval(F.col('target.accepted_date_key'), F.col('target.accepted_time_key'), F.col('source.event_timestamp'))\n",
    "                    , 'package_weight': F.col('source.package_weight')\n",
    "                    , 'status': F.col('source.status')\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    def write_delivered_orders(self):\n",
    "        \n",
    "        delta_table = DeltaTable.forPath(self.spark, self.fact_orders_path)\n",
    "        delivered_orders_df = self.joined_orders.where(F.col('status') == 'delivered')\n",
    "\n",
    "        (\n",
    "            delta_table.alias('target')\n",
    "            .merge(\n",
    "                source = delivered_orders_df.alias('source')\n",
    "                , condition = (F.col('source.order_id') == F.col('target.order_id'))\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                set = {\n",
    "                    'user_key': F.col('source.user_key')\n",
    "                    , 'pick_up_location_key': F.col('source.pick_up_location_key')\n",
    "                    , 'delivery_location_key': F.col('source.delivery_location_key')\n",
    "                    , 'package_description': F.col('source.package_description')\n",
    "                    , 'delivered_date_key': F.date_format(F.col('source.event_timestamp'), 'yyyyMMdd').cast(T.LongType())\n",
    "                    , 'delivered_time_key': F.date_format(F.col('source.event_timestamp'), 'HH:mm:ss')\n",
    "                    , 'in_transit_to_delivered_lag': self.make_interval(F.col('target.in_transit_date_key'), F.col('target.in_transit_time_key'), F.col('source.event_timestamp'))\n",
    "                    , 'delivered_and_delivery_difference': self.make_interval(F.col('target.delivery_date_key'), F.col('target.delivered_time_key'), F.col('source.event_timestamp'))\n",
    "                    , 'package_weight': F.col('source.package_weight')\n",
    "                    , 'status': F.col('source.status')\n",
    "                }\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "fact_orders_temp = FactOrders()\n",
    "fact_orders_temp.joined_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f19aec39-eb29-45a4-8b0d-9711cbf78ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d503cc15-95d8-4b07-9856-e6992b734902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jovyan/work/config')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd().parent / 'config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e717cf-c942-411d-b4b8-a93b0e7b748c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adb7d7f-f70c-410f-aed3-207e8765b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = fact_orders_temp.app_config.spark\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87e48bc3-689e-4fa8-9428-40c17c308047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "|id |start_date         |end_date           |\n",
      "+---+-------------------+-------------------+\n",
      "|1  |2024-02-01 10:00:00|2024-02-03 12:00:00|\n",
      "|2  |2024-02-02 09:30:00|2024-02-02 11:35:00|\n",
      "|3  |2024-02-03 14:00:00|2024-02-03 16:45:00|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.StructType(), True),\n",
    "    T.StructField(\"start_date\", T.TimestampType(), True),\n",
    "    T.StructField(\"end_date\", T.TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, datetime(2024, 2, 1, 10, 0, 0), datetime(2024, 2, 3, 12, 0, 0)),\n",
    "    (2, datetime(2024, 2, 2, 9, 30, 0), datetime(2024, 2, 2, 11, 35, 0)),\n",
    "    (3, datetime(2024, 2, 3, 14, 0, 0), datetime(2024, 2, 3, 16, 45, 0))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=[\"id\", \"start_date\", \"end_date\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63d5756b-ec37-4d16-941e-9c18be70ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('delta').option('path', '/test_table').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "611700c1-2cab-4b53-bb51-87f4714c9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |start_date         |end_date           |interval                           |make_interval(0, 0, 0, extract(D FROM lateralAliasReference(interval)), extract(H FROM lateralAliasReference(interval)), extract(m FROM lateralAliasReference(interval)), extract(s FROM lateralAliasReference(interval)))|\n",
      "+---+-------------------+-------------------+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |2024-02-01 10:00:00|2024-02-03 12:00:00|INTERVAL '2 02:00:00' DAY TO SECOND|2 days 2 hours                                                                                                                                                                                                            |\n",
      "|2  |2024-02-02 09:30:00|2024-02-02 11:35:00|INTERVAL '0 02:05:00' DAY TO SECOND|2 hours 5 minutes                                                                                                                                                                                                         |\n",
      "|3  |2024-02-03 14:00:00|2024-02-03 16:45:00|INTERVAL '0 02:45:00' DAY TO SECOND|2 hours 45 minutes                                                                                                                                                                                                        |\n",
      "+---+-------------------+-------------------+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col('*')\n",
    "    , (F.col('end_date') - F.col('start_date')).alias('interval')\n",
    "    , F.make_interval(\n",
    "        days = F.extract(F.lit('D'), F.col('interval'))\n",
    "        , hours = F.extract(F.lit('H'), F.col('interval'))\n",
    "        , mins = F.extract(F.lit('m'), F.col('interval'))\n",
    "        , secs = F.extract(F.lit('s'), F.col('interval'))\n",
    "    )\n",
    "\n",
    "    \n",
    ").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec85209b-0a22-4222-ba5e-0675a03e1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|r                                          |\n",
      "+-------------------------------------------+\n",
      "|1 days 12 hours 30 minutes 1.001001 seconds|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = fact_orders_temp.app_config.spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n",
    "    [\"year\", \"month\", \"week\", \"day\", \"hour\", \"min\", \"sec\"])\n",
    "df.select(\n",
    "    F.make_interval(days=df.day, hours=df.hour, mins=df.min, secs=df.sec).alias('r').cast(T.StringType())\n",
    "    # F.make_dt_interval(df.day, df.hour, df.min, df.sec).alias('datetime')\n",
    "    # F.make_ym_interval(df.year, df.month).alias('yearmonth')\n",
    ").show(20, False)\n",
    "# .write.mode('overwrite').format('delta').option('path', 'temp').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb10146a-1f5c-4ab7-ad3d-5fdbc3b08e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"2024-02-01 12:30:01\",),\n",
    "    (\"2024-02-02 14:45:30\",)\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"created_order_time\"]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = fact_orders_temp.app_config.spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the string column to a timestamp type\n",
    "df = df.withColumn(\"created_order_time\", F.to_timestamp(\"created_order_time\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99df3344-18b2-4169-b401-cae983939e77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      2\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     , F\u001b[38;5;241m.\u001b[39mdate_format(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myyyyMMdd\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(T\u001b[38;5;241m.\u001b[39mLongType())\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_date_key\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     , F\u001b[38;5;241m.\u001b[39mdate_format(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHH:mm:ss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time_key\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     , F\u001b[38;5;241m.\u001b[39mextract(F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m         F\u001b[38;5;241m.\u001b[39mto_timestamp(F\u001b[38;5;241m.\u001b[39mconcat(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_date_key\u001b[39m\u001b[38;5;124m'\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) ,F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time_key\u001b[39m\u001b[38;5;124m'\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myyyyMMdd HH:mm:ss\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mmake_interval(F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m0\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m0\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m0\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m0\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m15\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m30\u001b[39m), F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_order_time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     )\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m )\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col('created_order_time')\n",
    "    , F.date_format(F.col('created_order_time'), 'yyyyMMdd').cast(T.LongType()).alias('created_order_date_key')\n",
    "    , F.date_format(F.col('created_order_time'), \"HH:mm:ss\").alias('created_order_time_key')\n",
    "    , F.extract(F.lit('H'),\n",
    "        F.to_timestamp(F.concat(F.col('created_order_date_key'), F.lit(' ') ,F.col('created_order_time_key')), 'yyyyMMdd HH:mm:ss') \n",
    "        + F.make_interval(F.lit(0), F.lit(0), F.lit(0), F.lit(0), F.lit(15), F.lit(30), F.lit(1))\n",
    "        - F.col('created_order_time')\n",
    "    ).alias('temp')\n",
    "    \n",
    ").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5da5590-2ef0-45c7-8324-85a28c2f30c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.types' has no attribute 'IntervalType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntervalType\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.types' has no attribute 'IntervalType'"
     ]
    }
   ],
   "source": [
    "T.IntervalType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "975b49f7-029f-4414-917d-56fd671cc748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'make_interval(0, 0, 0, 20, 0, 0, 0)'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.make_interval(days = F.lit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cca86135-d10f-4f45-aaec-1abf8c3cb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').format('delta').option('path', '/temp').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19022f01-cb92-44cf-ad21-7dd86324d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EnrichedDataFactory at 0x7f7148a108d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4651a9a4-0901-4700-be6c-72f660d36c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first().Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34f5c8cb-1494-426f-bcfa-04844df10707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 456\n"
     ]
    }
   ],
   "source": [
    "def test(var1, var2):\n",
    "    print(var1, var2)\n",
    "test(**{'var1': '123', 'var2': 456})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "06192f1c-4ec0-4654-ab6b-4f5427fcdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnrichedDataFactory:\n",
    "    def __init__(self, config = Config):\n",
    "        self.app_config = config\n",
    "        self.raw_data = RawDataFactory(self.app_config)\n",
    "        \n",
    "        self.logger = self.app_config.logger\n",
    "        \n",
    "        self.configs = self.app_config.enriched_configs\n",
    "        self.base_path = self.configs.get('base_path')\n",
    "        self.mode = self.configs.get('mode', 'append')\n",
    "        self.format = self.configs.get('format', 'parquet')\n",
    "        self.partition_columns = self.configs.get('partition_columns')\n",
    "        self.compression = self.configs.get('compression')\n",
    "        self.extra_configs = self.configs.get('configs')\n",
    "        \n",
    "    @DecoratorFactory.decorate_select_columns\n",
    "    def _select_columns(self, raw_df):\n",
    "        new_raw_df = raw_df.select(\n",
    "            F.col('after.*')\n",
    "            , F.col('op')\n",
    "            , F.col('source.ts_ms')\n",
    "            , F.from_utc_timestamp(F.to_timestamp(F.col('source.ts_ms') / 1000), 'Asia/Ho_Chi_Minh').alias('event_timestamp')\n",
    "            , F.col('year')\n",
    "            , F.col('month')\n",
    "            , F.col('day')\n",
    "        )\n",
    "        return new_raw_df\n",
    "\n",
    "    \n",
    "    def _convert_iso_string_to_timestamp(self, iso_string):\n",
    "        try:\n",
    "            timestamp = F.from_utc_timestamp(F.to_timestamp(iso_string, \"yyyy-MM-dd'T'HH:mm:ss'Z'\"), 'Asia/Ho_Chi_Minh')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error parsing date: {e}')\n",
    "            raise\n",
    "        return timestamp\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    @DecoratorFactory.decorate_get_enriched_df\n",
    "    def enriched_users_df(self):\n",
    "        \n",
    "        self.logger.info('Starting transformation of raw users data.')\n",
    "        \n",
    "        new_users_df = self._select_columns(self.raw_data.raw_users_df)\n",
    "        cast_users_df = (\n",
    "            new_users_df\n",
    "            .withColumn(\n",
    "                'created_at'\n",
    "                , self._convert_iso_string_to_timestamp(F.col('created_at'))\n",
    "            )\n",
    "        )\n",
    "        self.logger.info('Completed transformation of raw users data.')\n",
    "        \n",
    "        return cast_users_df\n",
    "        \n",
    "    @property\n",
    "    @DecoratorFactory.decorate_get_enriched_df\n",
    "    def enriched_drivers_df(self):\n",
    "        \n",
    "        self.logger.info('Starting transformation of raw drivers data.')\n",
    "        \n",
    "        new_drivers_df = self._select_columns(self.raw_data.raw_drivers_df)\n",
    "        \n",
    "        self.logger.info('Completed transformation of raw drivers data.')\n",
    "        \n",
    "        return new_drivers_df\n",
    "    \n",
    "    @property\n",
    "    @DecoratorFactory.decorate_get_enriched_df\n",
    "    def enriched_orders_df(self):\n",
    "        \n",
    "        self.logger.info('Starting transformation of raw drivers data.')\n",
    "        \n",
    "        new_orders_df = self._select_columns(self.raw_data.raw_orders_df)\n",
    "        cast_orders_df = (\n",
    "            new_orders_df\n",
    "            .withColumn(\n",
    "                'delivery_time'\n",
    "                , self._convert_iso_string_to_timestamp(F.col('delivery_time'))\n",
    "            )\n",
    "            .withColumn(\n",
    "               'created_at'\n",
    "                , self._convert_iso_string_to_timestamp(F.col('created_at'))\n",
    "            )\n",
    "        )\n",
    "        self.logger.info('Completed transformation of raw orders data.')\n",
    "        return cast_orders_df\n",
    "        \n",
    "    @property\n",
    "    @DecoratorFactory.decorate_get_enriched_df\n",
    "    def enriched_payments_df(self):\n",
    "        self.logger.info('Starting transformation of raw payments data.')\n",
    "        \n",
    "        new_payments_df = self._select_columns(self.raw_data.raw_payments_df)\n",
    "        \n",
    "        cast_payments_df = (\n",
    "            new_payments_df\n",
    "            .withColumn(\n",
    "                'payment_date'\n",
    "                , self._convert_iso_string_to_timestamp(F.col('payment_date'))\n",
    "            )\n",
    "        )\n",
    "        self.logger.info('Completed transformation of raw payments data.')\n",
    "        \n",
    "        return cast_payments_df\n",
    "        \n",
    "    @property\n",
    "    @DecoratorFactory.decorate_get_enriched_df\n",
    "    def enriched_shipments_df(self):\n",
    "        self.logger.info(\"Starting transformation of raw shipments data.\")\n",
    "        \n",
    "        new_shipments_df = self._select_columns(self.raw_data.raw_shipments_df)\n",
    "        cast_shipments_df = (\n",
    "            new_shipments_df\n",
    "            .withColumn(\n",
    "                'estimated_delivery_time'\n",
    "                , self._convert_iso_string_to_timestamp(F.col('estimated_delivery_time'))\n",
    "            )\n",
    "        )\n",
    "        self.logger.info('Completed transformation of raw shipments data.')\n",
    "        \n",
    "        return cast_shipments_df\n",
    "        \n",
    "    @property\n",
    "    def data_lake_loader(self):\n",
    "        return DataLakeLoader(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b028fa8d-20ac-4399-b020-49b5152a8d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9937a5-af09-4bdd-925f-f1946280dbe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'decorator_factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(Path\u001b[38;5;241m.\u001b[39mcwd() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecorator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecorator_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoratorFactory\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnrichedDataLoader\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_path, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m, partition_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, compression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, extra_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m ):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'decorator_factory'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path.cwd() / 'decorator')\n",
    "\n",
    "from decorator_factory import DecoratorFactory\n",
    "\n",
    "class EnrichedDataLoader:\n",
    "    def __init__(self, base_path, mode = 'append', format = 'parquet', partition_columns = None, compression = None, extra_configs = None ):\n",
    "        self.base_path = base_path\n",
    "        self.mode = mode\n",
    "        self.format = format\n",
    "        self.partition_columns = partition_columns\n",
    "        self.compression = compression\n",
    "        self.extra_cnfigs = extra_configs\n",
    "\n",
    "    @DecoratorFactory.decorate_load_enriched_data\n",
    "    def load_enriched_data(self, df: DataFrame, df_name: str) -> None:\n",
    "        \n",
    "        output_path = f'{self.base_path}/{df_name}'\n",
    "        writer = df.write.mode(self.mode)\n",
    "\n",
    "        if self.compression:\n",
    "            writer = writer.option(\"compression\", self.compression)\n",
    "        if self.partition_columns:\n",
    "            writer = writer.partitionBy(self.partition_columns)\n",
    "        if self.extra_configs:\n",
    "            writer = writer.options(**self.extra_configs)\n",
    "        \n",
    "        writer.format(self.format).save(output_path)\n",
    "        self.logger.info(f\"Path: {output_path}\")\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e979b94f-625b-446a-92d2-b4f2ef8d6530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 09:41:02,412 - __main__ - INFO - Starting transformation of raw users data.\n",
      "2025-01-24 09:41:02,413 - __main__ - INFO - Starting to fetch raw users data from HDFS.\n",
      "2025-01-24 09:41:02,414 - __main__ - INFO - DataFrame: users\n",
      "2025-01-24 09:41:02,414 - __main__ - INFO - Executing function: _get_raw_data\n",
      "2025-01-24 09:41:02,416 - __main__ - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Users\n",
      "2025-01-24 09:41:02,626 - __main__ - INFO - Successfully fetched raw users data.\n",
      "2025-01-24 09:41:02,627 - __main__ - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-01-24 09:41:02,698 - __main__ - INFO - Completed transformation of raw users data.\n",
      "2025-01-24 09:41:02,699 - __main__ - INFO - Starting transformation of raw drivers data.\n",
      "2025-01-24 09:41:02,700 - __main__ - INFO - Starting to fetch raw drivers data from HDFS.\n",
      "2025-01-24 09:41:02,701 - __main__ - INFO - DataFrame: drivers\n",
      "2025-01-24 09:41:02,702 - __main__ - INFO - Executing function: _get_raw_data\n",
      "2025-01-24 09:41:02,703 - __main__ - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Drivers\n",
      "2025-01-24 09:41:02,759 - __main__ - INFO - Successfully fetched raw drivers data.\n",
      "2025-01-24 09:41:02,760 - __main__ - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-01-24 09:41:02,790 - __main__ - INFO - Completed transformation of raw drivers data.\n",
      "2025-01-24 09:41:02,792 - __main__ - INFO - Starting transformation of raw drivers data.\n",
      "2025-01-24 09:41:02,793 - __main__ - INFO - Starting to fetch raw orders data from HDFS.\n",
      "2025-01-24 09:41:02,793 - __main__ - INFO - DataFrame: orders\n",
      "2025-01-24 09:41:02,794 - __main__ - INFO - Executing function: _get_raw_data\n",
      "2025-01-24 09:41:02,795 - __main__ - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Orders\n",
      "2025-01-24 09:41:02,874 - __main__ - INFO - Successfully fetched raw orders data.\n",
      "2025-01-24 09:41:02,876 - __main__ - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-01-24 09:41:02,928 - __main__ - INFO - Completed transformation of raw orders data.\n",
      "2025-01-24 09:41:02,929 - __main__ - INFO - Starting transformation of raw payments data.\n",
      "2025-01-24 09:41:02,930 - __main__ - INFO - Starting to fetch raw payments data from HDFS.\n",
      "2025-01-24 09:41:02,931 - __main__ - INFO - DataFrame: payments\n",
      "2025-01-24 09:41:02,931 - __main__ - INFO - Executing function: _get_raw_data\n",
      "2025-01-24 09:41:02,932 - __main__ - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Payments\n",
      "2025-01-24 09:41:02,990 - __main__ - INFO - Successfully fetched raw payments data.\n",
      "2025-01-24 09:41:02,991 - __main__ - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-01-24 09:41:03,031 - __main__ - INFO - Completed transformation of raw payments data.\n",
      "2025-01-24 09:41:03,032 - __main__ - INFO - Starting transformation of raw shipments data.\n",
      "2025-01-24 09:41:03,033 - __main__ - INFO - Starting to fetch raw shipments data from HDFS.\n",
      "2025-01-24 09:41:03,034 - __main__ - INFO - DataFrame: shipments\n",
      "2025-01-24 09:41:03,035 - __main__ - INFO - Executing function: _get_raw_data\n",
      "2025-01-24 09:41:03,036 - __main__ - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Shipments\n",
      "2025-01-24 09:41:03,087 - __main__ - INFO - Successfully fetched raw shipments data.\n",
      "2025-01-24 09:41:03,088 - __main__ - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-01-24 09:41:03,132 - __main__ - INFO - Completed transformation of raw shipments data.\n",
      "2025-01-24 09:41:03,133 - __main__ - INFO - Starting to load users to enriched layer.\n",
      "2025-01-24 09:41:03,134 - __main__ - INFO - DataFrame: users\n",
      "2025-01-24 09:41:03,135 - __main__ - INFO - Executing function: load_enriched_data\n",
      "2025-01-24 09:41:04,291 - __main__ - INFO - Path: /enriched/transactional/mysql/logistics/users\n",
      "2025-01-24 09:41:04,292 - __main__ - INFO - Successfully loaded users to enriched layer.\n",
      "2025-01-24 09:41:04,293 - __main__ - INFO - Starting to load drivers to enriched layer.\n",
      "2025-01-24 09:41:04,294 - __main__ - INFO - DataFrame: drivers\n",
      "2025-01-24 09:41:04,295 - __main__ - INFO - Executing function: load_enriched_data\n",
      "2025-01-24 09:41:04,662 - __main__ - INFO - Path: /enriched/transactional/mysql/logistics/drivers\n",
      "2025-01-24 09:41:04,663 - __main__ - INFO - Successfully loaded drivers to enriched layer.\n",
      "2025-01-24 09:41:04,664 - __main__ - INFO - Starting to load orders to enriched layer.\n",
      "2025-01-24 09:41:04,665 - __main__ - INFO - DataFrame: orders\n",
      "2025-01-24 09:41:04,666 - __main__ - INFO - Executing function: load_enriched_data\n",
      "2025-01-24 09:41:05,800 - __main__ - INFO - Path: /enriched/transactional/mysql/logistics/orders\n",
      "2025-01-24 09:41:05,801 - __main__ - INFO - Successfully loaded orders to enriched layer.\n",
      "2025-01-24 09:41:05,802 - __main__ - INFO - Starting to load payments to enriched layer.\n",
      "2025-01-24 09:41:05,803 - __main__ - INFO - DataFrame: payments\n",
      "2025-01-24 09:41:05,804 - __main__ - INFO - Executing function: load_enriched_data\n",
      "2025-01-24 09:41:06,130 - __main__ - INFO - Path: /enriched/transactional/mysql/logistics/payments\n",
      "2025-01-24 09:41:06,131 - __main__ - INFO - Successfully loaded payments to enriched layer.\n",
      "2025-01-24 09:41:06,132 - __main__ - INFO - Starting to load shipments to enriched layer.\n",
      "2025-01-24 09:41:06,133 - __main__ - INFO - DataFrame: shipments\n",
      "2025-01-24 09:41:06,135 - __main__ - INFO - Executing function: load_enriched_data\n",
      "2025-01-24 09:41:06,433 - __main__ - INFO - Path: /enriched/transactional/mysql/logistics/shipments\n",
      "2025-01-24 09:41:06,434 - __main__ - INFO - Successfully loaded shipments to enriched layer.\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    config = Config()\n",
    "    enriched_data = EnrichedDataFactory(config)\n",
    "    dfs = []\n",
    "    dfs.append((enriched_data.enriched_users_df, 'users'))\n",
    "    dfs.append((enriched_data.enriched_drivers_df, 'drivers'))\n",
    "    dfs.append((enriched_data.enriched_orders_df, 'orders'))\n",
    "    dfs.append((enriched_data.enriched_payments_df, 'payments'))\n",
    "    dfs.append((enriched_data.enriched_shipments_df, 'shipments'))\n",
    "\n",
    "    for df, df_name in dfs:\n",
    "        enriched_data.data_lake_loader.load_enriched_data(df, df_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab0eae-2c59-4e62-be58-d4478f2f0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.logger.info('8888888888')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32245f95-56eb-41e8-8dd6-267da5fc4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = config.spark.read.format('parquet').option('path', '/raw/transactional/mysql/logistics/topics/logistics_src.logistics.Payments/').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471e419-1b24-44c4-bee4-267c8a624256",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b567f-778c-4e02-b79c-d8cb28a0936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.DecimalType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78daeb9-eab8-4a5e-9c58-8162ad074e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp.select(F.col('after.amount').cast(T.StringType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddc07b-7af0-4784-a9fc-193d320a4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b913b-56f9-4494-a38c-81afaf4bf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.now() - datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed30cc-c460-48db-a5f5-e223b8d74291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( var: Optional[str] = None) -> str:\n",
    "    print(type(var))\n",
    "test(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500213fb-2ba0-422b-87ce-36eb4d9413d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "month = 1\n",
    "day = 1\n",
    "print(f'/{year}/{month:0>2}/{day:0>2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade27027-eb4a-4549-becd-fd1299e3115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.spark.read.format('avro').option('path', path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7979c7-cdbe-4ea3-824a-60186e6c22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/work/logistics_project_v2/test/test/logistics_src.logistics.Drivers+0+0000000000+0000000019.avro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460dd1c2-de10-4187-9ecf-2ef60dd4d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['avro.codec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750db2a-f11e-44a1-bba4-6b271318ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(func):\n",
    "    def wrappler(*args, **kwargs):\n",
    "        t = func(*args, **kwargs)\n",
    "        return t\n",
    "    return wrappler\n",
    "\n",
    "@test1\n",
    "def cai(n):\n",
    "    return n\n",
    "cai(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22109561-436b-4580-bb3f-860d7c3a9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59093af-ae72-4211-af89-c7dae7351449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_authorization(f):\n",
    "    def wrapper(self, var1):\n",
    "        print(self.url)\n",
    "        print(var1)\n",
    "        return f(self, var1)\n",
    "    return wrapper\n",
    "\n",
    "class Client(object):\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    @check_authorization\n",
    "    def get(self, var1):\n",
    "        print('get')\n",
    "\n",
    "Client('url 123').get(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994affe-0021-4fb7-8f97-d208181f4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb02eb9-84a4-4ec8-9dbc-7631ae915dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ad90f-b21c-407c-9f03-ac2e9d8e9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"Wrapper function.\"\"\"\n",
    "        print(\"Before the function call\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"After the function call\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def greet(name):\n",
    "    \"\"\"Greet a person by name.\"\"\"\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "print(greet.__name__)   # Output: wrapper (instead of 'greet')\n",
    "print(greet.__doc__)    # Output: Wrapper function. (original docstring is lost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e4b18-4f94-4361-b14e-564f8fbe8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def my_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"Wrapper function.\"\"\"\n",
    "        print(\"Before the function call\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"After the function call\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def greet(name):\n",
    "    \"\"\"Greet a person by name.\"\"\"\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "print(greet.__name__)   # Output: greet (metadata preserved)\n",
    "print(greet.__doc__)    # Output: Greet a person by name. (original docstring preserved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdb84-3564-4d10-9f02-d989565adc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171779cc-a58e-408c-8ef9-9619458f5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 29),\n",
    "    (2, \"Bob\", 35),\n",
    "    (3, \"Charlie\", 23)\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = config.spark.createDataFrame(data, schema=columns)\n",
    "df = df.withColumn(\n",
    "    'year', F.lit(2025)\n",
    ").withColumn(\n",
    "    'month', F.lit(1)\n",
    ").withColumn(\n",
    "    'day', F.lit(24)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e560188-9328-486d-8dec-7f397543a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\n",
    "    'day'\n",
    "    , F.when(F.col('id') == 3, F.lit(23)).otherwise(F.col('day'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff2a91-5dc1-4cda-9005-623aa5553e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\n",
    "    'day'\n",
    "    , F.lit(23)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994062d-618c-4059-a866-9930535faca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82670f46-fc96-4a38-a31a-15c2d94368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/jovyan/work/logistics_project_v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc097149-3f89-4f25-b47b-9a53b84dd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_config = {'header': True}\n",
    "(\n",
    "    df.write.mode('overwrite')\n",
    "    .format('csv')\n",
    "    .partitionBy(['year', 'month', 'day'])\n",
    "    .options(**temp_config)\n",
    "    .save('file:////home/jovyan/work/logistics_project_v2/df')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d3755-7958-4500-8edd-94445fff2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ba80e-5b1c-45de-b5a8-fb7df264fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d3dea-8e9d-4393-bceb-98a74a710692",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.spark.read.format('csv').option('path', 'file:///home/jovyan/work/logistics_project_v2/df').load().where(F.col('year') == 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26515771-d8f8-4f5d-951d-fc89506160d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.spark.read.format('avro').option('path', '/raw/transactional/mysql/logistics/topics/logistics_src.logistics.Users').load().where(F.col('year') == 2025).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920f8123-7403-461d-a52c-b83140a2b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aab87eb-b9d1-4574-bd0a-de8646b6f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jovyan/work/logistics_project_v2/123/123')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd() / '123/123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a65efbd-14a7-4603-aa0d-14cb2445fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b5c1d4-9b4e-47a5-8070-00304a612bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract.raw_data_factory import RawDataFactory\n",
    "from transform.enriched_data_factory import EnrichedDataFactory\n",
    "from load.data_lake_loader import DataLakeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8572f0-b242-47d2-a230-84210c60c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a741a42e-6bf4-4b45-aec0-104d85b6ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import raw_to_enriched_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7168df5f-93be-4fa1-add0-26ee303f6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 06:54:07,618 - config - INFO - Starting transformation of raw users data.\n",
      "2025-02-01 06:54:07,619 - config - INFO - Starting to fetch raw users data from HDFS.\n",
      "2025-02-01 06:54:07,620 - config - INFO - DataFrame: users\n",
      "2025-02-01 06:54:07,621 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:54:07,621 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Users\n",
      "2025-02-01 06:54:07,686 - config - INFO - Successfully fetched raw users data.\n",
      "2025-02-01 06:54:07,687 - config - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-01 06:54:07,729 - config - INFO - Completed transformation of raw users data.\n",
      "2025-02-01 06:54:07,730 - config - INFO - Starting transformation of raw drivers data.\n",
      "2025-02-01 06:54:07,731 - config - INFO - Starting to fetch raw drivers data from HDFS.\n",
      "2025-02-01 06:54:07,732 - config - INFO - DataFrame: drivers\n",
      "2025-02-01 06:54:07,733 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:54:07,733 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Drivers\n",
      "2025-02-01 06:54:07,774 - config - INFO - Successfully fetched raw drivers data.\n",
      "2025-02-01 06:54:07,775 - config - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-01 06:54:07,808 - config - INFO - Completed transformation of raw drivers data.\n",
      "2025-02-01 06:54:07,809 - config - INFO - Starting transformation of raw drivers data.\n",
      "2025-02-01 06:54:07,810 - config - INFO - Starting to fetch raw orders data from HDFS.\n",
      "2025-02-01 06:54:07,811 - config - INFO - DataFrame: orders\n",
      "2025-02-01 06:54:07,812 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:54:07,813 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Orders\n",
      "2025-02-01 06:54:07,885 - config - INFO - Successfully fetched raw orders data.\n",
      "2025-02-01 06:54:07,886 - config - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-01 06:54:07,934 - config - INFO - Completed transformation of raw orders data.\n",
      "2025-02-01 06:54:07,935 - config - INFO - Starting transformation of raw payments data.\n",
      "2025-02-01 06:54:07,936 - config - INFO - Starting to fetch raw payments data from HDFS.\n",
      "2025-02-01 06:54:07,937 - config - INFO - DataFrame: payments\n",
      "2025-02-01 06:54:07,938 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:54:07,939 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Payments\n",
      "2025-02-01 06:54:07,981 - config - INFO - Successfully fetched raw payments data.\n",
      "2025-02-01 06:54:07,983 - config - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-01 06:54:08,010 - config - INFO - Completed transformation of raw payments data.\n",
      "2025-02-01 06:54:08,011 - config - INFO - Starting transformation of raw shipments data.\n",
      "2025-02-01 06:54:08,012 - config - INFO - Starting to fetch raw shipments data from HDFS.\n",
      "2025-02-01 06:54:08,013 - config - INFO - DataFrame: shipments\n",
      "2025-02-01 06:54:08,014 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:54:08,015 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Shipments\n",
      "2025-02-01 06:54:08,058 - config - INFO - Successfully fetched raw shipments data.\n",
      "2025-02-01 06:54:08,059 - config - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-01 06:54:08,103 - config - INFO - Completed transformation of raw shipments data.\n",
      "2025-02-01 06:54:08,104 - config - INFO - Starting to load users to enriched layer.\n",
      "2025-02-01 06:54:08,105 - config - INFO - DataFrame: users\n",
      "2025-02-01 06:54:08,106 - config - INFO - Executing function: load_enriched_data\n",
      "2025-02-01 06:54:09,127 - config - INFO - Path: /enriched/transactional/mysql/logistics/users\n",
      "2025-02-01 06:54:09,129 - config - INFO - Successfully loaded users to enriched layer.\n",
      "2025-02-01 06:54:09,130 - config - INFO - Starting to load drivers to enriched layer.\n",
      "2025-02-01 06:54:09,131 - config - INFO - DataFrame: drivers\n",
      "2025-02-01 06:54:09,132 - config - INFO - Executing function: load_enriched_data\n",
      "2025-02-01 06:54:09,984 - config - INFO - Path: /enriched/transactional/mysql/logistics/drivers\n",
      "2025-02-01 06:54:09,986 - config - INFO - Successfully loaded drivers to enriched layer.\n",
      "2025-02-01 06:54:09,988 - config - INFO - Starting to load orders to enriched layer.\n",
      "2025-02-01 06:54:09,989 - config - INFO - DataFrame: orders\n",
      "2025-02-01 06:54:09,991 - config - INFO - Executing function: load_enriched_data\n",
      "2025-02-01 06:54:10,891 - config - INFO - Path: /enriched/transactional/mysql/logistics/orders\n",
      "2025-02-01 06:54:10,892 - config - INFO - Successfully loaded orders to enriched layer.\n",
      "2025-02-01 06:54:10,893 - config - INFO - Starting to load payments to enriched layer.\n",
      "2025-02-01 06:54:10,894 - config - INFO - DataFrame: payments\n",
      "2025-02-01 06:54:10,895 - config - INFO - Executing function: load_enriched_data\n",
      "2025-02-01 06:54:11,619 - config - INFO - Path: /enriched/transactional/mysql/logistics/payments\n",
      "2025-02-01 06:54:11,620 - config - INFO - Successfully loaded payments to enriched layer.\n",
      "2025-02-01 06:54:11,621 - config - INFO - Starting to load shipments to enriched layer.\n",
      "2025-02-01 06:54:11,622 - config - INFO - DataFrame: shipments\n",
      "2025-02-01 06:54:11,623 - config - INFO - Executing function: load_enriched_data\n",
      "2025-02-01 06:53:13,476 - config - INFO - Path: /enriched/transactional/mysql/logistics/shipments\n",
      "2025-02-01 06:53:13,479 - config - INFO - Successfully loaded shipments to enriched layer.\n"
     ]
    }
   ],
   "source": [
    "raw_to_enriched_pipeline.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff43d805-6253-4812-8553-632d7c04f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8ae975-5ab4-4a85-ad12-5b32ebbfc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56a888e-e7a8-45fa-939f-a9eb2c780e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime(2025, 1, 22, 13, 34, 8, 974981, tzinfo=Timezone('Asia/Ho_Chi_Minh'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pendulum.now(tz='Asia/Ho_Chi_Minh').subtract(days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a587668-1368-4eb0-b7f4-37b60a8df9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime(2025, 1, 31, 0, 0, 0, tzinfo=Timezone('Etc/UTC'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pendulum.yesterday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5758c4f0-00bd-4991-9602-e378ba9155c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4006984-69f8-430d-b6f6-207dcb09eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.enriched_data_factory import EnrichedDataFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f072fea6-d5d7-4a99-96d4-483cc7e150eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime(2025, 2, 1, 13, 42, 53, 834324, tzinfo=Timezone('Asia/Ho_Chi_Minh'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pendulum.now(tz='Asia/Ho_Chi_Minh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b2f7c1-5202-41c5-b30e-4183ba985ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': DateTime(2025, 1, 22, 13, 43, 46, 869480, tzinfo=Timezone('Asia/Ho_Chi_Minh')),\n",
       " 'year': 2025,\n",
       " 'month': 1,\n",
       " 'day': 22}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EnrichedDataFactory().app_config.previous_date_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbfaa47-def8-4a70-8914-42bd38b7a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 06:46:46,614 - config - INFO - Starting to fetch raw users data from HDFS.\n",
      "2025-02-01 06:46:46,616 - config - INFO - DataFrame: users\n",
      "2025-02-01 06:46:46,618 - config - INFO - Executing function: _get_raw_data\n",
      "2025-02-01 06:46:46,620 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Users\n",
      "2025-02-01 06:46:46,858 - config - INFO - Successfully fetched raw users data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------+---+-------------+----------------+-------------------+----+-----+---+\n",
      "|before|               after|              source|transaction| op|        ts_ms|           ts_us|              ts_ns|year|month|day|\n",
      "+------+--------------------+--------------------+-----------+---+-------------+----------------+-------------------+----+-----+---+\n",
      "|  NULL|{121, User_121, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339947|1737525339947311|1737525339947311698|2025|    1| 22|\n",
      "|  NULL|{122, User_122, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339947|1737525339947497|1737525339947497410|2025|    1| 22|\n",
      "|  NULL|{123, User_123, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339947|1737525339947660|1737525339947660791|2025|    1| 22|\n",
      "|  NULL|{124, User_124, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339947|1737525339947822|1737525339947822509|2025|    1| 22|\n",
      "|  NULL|{125, User_125, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339948|1737525339948176|1737525339948176481|2025|    1| 22|\n",
      "|  NULL|{126, User_126, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339948|1737525339948699|1737525339948699061|2025|    1| 22|\n",
      "|  NULL|{127, User_127, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339950|1737525339950391|1737525339950391899|2025|    1| 22|\n",
      "|  NULL|{128, User_128, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339952|1737525339952513|1737525339952513952|2025|    1| 22|\n",
      "|  NULL|{129, User_129, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339953|1737525339953172|1737525339953172180|2025|    1| 22|\n",
      "|  NULL|{130, User_130, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339954|1737525339954414|1737525339954414975|2025|    1| 22|\n",
      "|  NULL|{131, User_131, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339954|1737525339954771|1737525339954771199|2025|    1| 22|\n",
      "|  NULL|{132, User_132, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339954|1737525339954948|1737525339954948533|2025|    1| 22|\n",
      "|  NULL|{133, User_133, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339955|1737525339955206|1737525339955206511|2025|    1| 22|\n",
      "|  NULL|{134, User_134, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339955|1737525339955576|1737525339955576559|2025|    1| 22|\n",
      "|  NULL|{135, User_135, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339955|1737525339955818|1737525339955818677|2025|    1| 22|\n",
      "|  NULL|{136, User_136, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339955|1737525339955977|1737525339955977092|2025|    1| 22|\n",
      "|  NULL|{137, User_137, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339956|1737525339956257|1737525339956257056|2025|    1| 22|\n",
      "|  NULL|{138, User_138, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339956|1737525339956530|1737525339956530509|2025|    1| 22|\n",
      "|  NULL|{139, User_139, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339958|1737525339958369|1737525339958369065|2025|    1| 22|\n",
      "|  NULL|{140, User_140, u...|{3.0.5.Final, mys...|       NULL|  r|1737525339958|1737525339958836|1737525339958836396|2025|    1| 22|\n",
      "+------+--------------------+--------------------+-----------+---+-------------+----------------+-------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EnrichedDataFactory().raw_data.raw_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fd7c2c-23da-4996-833a-f72dbcc964b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 09:34:00,370 - config - INFO - Starting transformation of raw users data.\n",
      "2025-01-26 09:34:00,371 - config - INFO - Starting to fetch raw users data from HDFS.\n",
      "2025-01-26 09:34:00,371 - config - INFO - DataFrame: users\n",
      "2025-01-26 09:34:00,372 - config - INFO - Executing function: _get_raw_data\n",
      "2025-01-26 09:34:00,373 - config - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Users\n",
      "2025-01-26 09:34:04,164 - config - ERROR - Error while fetching users from raw layer.\n",
      "2025-01-26 09:34:04,166 - config - ERROR - DataFrame: users\n",
      "2025-01-26 09:34:04,167 - config - ERROR - Error: name 'F' is not defined\n",
      "2025-01-26 09:34:04,169 - config - ERROR - name 'F' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         enriched_data\u001b[38;5;241m.\u001b[39mdata_lake_loader\u001b[38;5;241m.\u001b[39mload_enriched_data(df, df_name)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m enriched_data \u001b[38;5;241m=\u001b[39m EnrichedDataFactory()\n\u001b[1;32m      4\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend((\u001b[43menriched_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menriched_users_df\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend((enriched_data\u001b[38;5;241m.\u001b[39menriched_drivers_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrivers\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend((enriched_data\u001b[38;5;241m.\u001b[39menriched_orders_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/work/logistics_project_v2/decorator/decorator_factory.py:44\u001b[0m, in \u001b[0;36mDecoratorFactory.decorate_get_enriched_df.<locals>.new_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_function\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         new_df \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(e)\n",
      "File \u001b[0;32m~/work/logistics_project_v2/transform/enriched_data_factory.py:58\u001b[0m, in \u001b[0;36mEnrichedDataFactory.enriched_users_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@DecoratorFactory\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_get_enriched_df\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menriched_users_df\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting transformation of raw users data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     new_users_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_columns(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_users_df\u001b[49m)\n\u001b[1;32m     59\u001b[0m     cast_users_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m         new_users_df\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleted transformation of raw users data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/work/logistics_project_v2/extract/raw_data_factory.py:49\u001b[0m, in \u001b[0;36mRawDataFactory.raw_users_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m raw_users_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m raw_users_data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistics_src.logistics.Users\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_users_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_users_data_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/logistics_project_v2/decorator/decorator_factory.py:14\u001b[0m, in \u001b[0;36mDecoratorFactory.decorate_get_raw_data.<locals>.new_function\u001b[0;34m(self, raw_name, raw_data_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecuting function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_data_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccessfully fetched raw \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/work/logistics_project_v2/extract/raw_data_factory.py:38\u001b[0m, in \u001b[0;36mRawDataFactory._get_raw_data\u001b[0;34m(self, raw_name, raw_data_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m raw_df_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_data_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDFS Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_format)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_df_path)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m---> 38\u001b[0m         (\u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_year)\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_month)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;241m&\u001b[39m (F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_day)\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def main() -> None:\n",
    "    \n",
    "    enriched_data = EnrichedDataFactory()\n",
    "    dfs = []\n",
    "    dfs.append((enriched_data.enriched_users_df, 'users'))\n",
    "    dfs.append((enriched_data.enriched_drivers_df, 'drivers'))\n",
    "    dfs.append((enriched_data.enriched_orders_df, 'orders'))\n",
    "    dfs.append((enriched_data.enriched_payments_df, 'payments'))\n",
    "    dfs.append((enriched_data.enriched_shipments_df, 'shipments'))\n",
    "\n",
    "    for df, df_name in dfs:\n",
    "        enriched_data.data_lake_loader.load_enriched_data(df, df_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67524748-3aa4-449e-a03c-62e1a8df5334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function delta.pip_utils.configure_spark_with_delta_pip(spark_session_builder: pyspark.sql.session.SparkSession.Builder, extra_packages: Optional[List[str]] = None) -> pyspark.sql.session.SparkSession.Builder>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b962105c-0a6e-4345-a457-ce3e9ebeea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows import process_enriched_shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074e06f8-00ef-4a9f-b490-c8d630f9a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 02:09:23,344 - logistics - INFO - Starting transformation of raw shipments data.\n",
      "2025-02-05 02:09:23,345 - logistics - INFO - Starting to fetch raw shipments data from HDFS.\n",
      "2025-02-05 02:09:23,346 - logistics - INFO - DataFrame: shipments\n",
      "2025-02-05 02:09:23,347 - logistics - INFO - Executing function: _extract_raw_data\n",
      "2025-02-05 02:09:23,348 - logistics - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Shipments\n",
      "2025-02-05 02:09:23,418 - logistics - INFO - Successfully fetched raw shipments data.\n",
      "2025-02-05 02:09:23,419 - logistics - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-05 02:09:23,473 - logistics - INFO - Completed transformation of raw shipments data.\n",
      "2025-02-05 02:09:24,005 - logistics - INFO - Path: /enriched/transactional/mysql/logistics/shipments\n"
     ]
    }
   ],
   "source": [
    "process_enriched_shipments.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f65c88-dd62-489c-bc02-8f79f1cffa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 02:08:52,326 - logistics - INFO - Starting transformation of raw drivers data.\n",
      "2025-02-05 02:08:52,327 - logistics - INFO - Starting to fetch raw orders data from HDFS.\n",
      "2025-02-05 02:08:52,328 - logistics - INFO - DataFrame: orders\n",
      "2025-02-05 02:08:52,329 - logistics - INFO - Executing function: _extract_raw_data\n",
      "2025-02-05 02:08:52,330 - logistics - INFO - HDFS Path: /raw/transactional/mysql/logistics/topics/logistics_src.logistics.Orders\n",
      "2025-02-05 02:08:52,434 - logistics - INFO - Successfully fetched raw orders data.\n",
      "2025-02-05 02:08:52,435 - logistics - INFO - Selecting columns: ['after.*', 'op', 'source.ts_ms', 'event_timestamp', 'year', 'month', 'day']\n",
      "2025-02-05 02:08:52,508 - logistics - INFO - Completed transformation of raw orders data.\n",
      "2025-02-05 02:08:54,100 - logistics - INFO - Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    }
   ],
   "source": [
    "process_enriched_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc37554-5981-475e-bfec-37f4ba0e65de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
