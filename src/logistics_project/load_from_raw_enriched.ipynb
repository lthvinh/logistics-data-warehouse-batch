{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29b699e-4ad1-4265-8e1d-5e7788b3ac13",
   "metadata": {},
   "source": [
    "# enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0351c6-fa8f-4e28-a8f2-451572e25da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.enriched import process_enriched_drivers, process_enriched_orders, process_enriched_payments, process_enriched_shipments, process_enriched_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf3455-6178-4b3c-a35f-9b63f1c931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_drivers.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c26d20-10d9-41a9-a5e3-20d156cef6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ac5c5-fc6c-4860-a2de-0b68b34e2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_payments.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084333b5-5fc4-43f3-92d0-39ca89bf0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_shipments.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788207f7-f3f9-42f1-b241-6c03687e7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_users.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe1541-2575-4d5d-ad6e-8590f0504e38",
   "metadata": {},
   "source": [
    "# curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d8d9e-3318-4c36-b852-1ec52014cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_dim_users, process_dim_drivers, process_dim_date, process_dim_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c9940-bf71-4359-9752-c1907e50dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dim_users.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54dbb70-dfc4-47b4-be1e-de076ea370f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dim_date.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac31e02-29bb-462b-9050-87159be0c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dim_locations.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a6557-1751-4aaa-a5fb-0e3577f4ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dim_drivers.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489215af-9b43-4283-b587-d94c4e000afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_fact_processing_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc498760-9893-4859-9248-8d1a09bf82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_fact_processing_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0a418-3418-47e0-9a10-06cc28ac6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders = (\n",
    "    spark.read.format('delta')\n",
    "    .option('path', '/curated/transactional/mysql/logistics/facts/fact_orders')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed83dd1-0064-40fd-9e6c-1281c69dd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc6e2e-4c89-4732-8ec1-f754daa8c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3c2b0-2a81-4b95-a329-1e2a8142fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self):\n",
    "        self.app_config = Config()\n",
    "        \n",
    "        #COMMON\n",
    "        self.spark = self.app_config.spark\n",
    "        self.logger = self.app_config.logger\n",
    "        \n",
    "        #ENRICHED CONFIGS\n",
    "        self.enriched_configs = self.app_config.enriched_configs\n",
    "        self.enriched_base_path = self.enriched_configs.get('base_path')\n",
    "        self.enriched_format = self.enriched_configs.get('format', 'parquet')\n",
    "\n",
    "        #PREVIOUS DATE DETAILS\n",
    "        self.previous_date_details = self.app_config.previous_date_details\n",
    "        self.previous_year = self.previous_date_details['year']\n",
    "        self.previous_month = self.previous_date_details['month']\n",
    "        self.previous_day = self.previous_date_details['day']\n",
    "\n",
    "        #ENRICHED DATA EXTRACTOR\n",
    "        self.enriched_data_extractor = EnrichedDataExtractor(\n",
    "            self.spark\n",
    "            , self.logger\n",
    "            , self.enriched_base_path\n",
    "            , self.enriched_format\n",
    "            , self.previous_year\n",
    "            , self.previous_month\n",
    "            , self.previous_day\n",
    "        )\n",
    "\n",
    "        #CURATED CONFIGS\n",
    "        self.curated_configs = self.app_config.curated_configs\n",
    "        self.dimension_base_path = self.curated_configs.get('dimension_base_path')\n",
    "        self.fact_base_path = self.curated_configs.get('fact_base_path')\n",
    "\n",
    "        #CURATED DATA LOADER\n",
    "        self.curated_data_loader = CuratedDataLoader()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec7979-aa7f-49eb-b546-78c465afe43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from delta import *\n",
    "from pyspark.sql.window import Window\n",
    "from extract.enriched_data_extractor import EnrichedDataExtractor\n",
    "from load.curated_data_loader import CuratedDataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d2979-84fd-476a-9db8-15ceac298394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactOrders(Table):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim_users_path = self.dimension_base_path + '/dim_users'\n",
    "        self.dim_locations_path = self.dimension_base_path + '/dim_locations'\n",
    "        self.fact_orders_path = self.fact_base_path + '/fact_orders'\n",
    "\n",
    "    @property\n",
    "    def dim_users(self):\n",
    "        dim_users = (\n",
    "            self.spark.read.format('delta').option('path', self.dim_users_path).load()\n",
    "            .where((F.col('is_current')))\n",
    "            .select(F.col('user_id'), F.col('user_key'))\n",
    "        )\n",
    "        return dim_users\n",
    "        \n",
    "    @property\n",
    "    def dim_locations(self):\n",
    "        dim_locations = self.spark.read.format('delta').option('path', self.dim_locations_path).load()\n",
    "        return dim_locations\n",
    "        \n",
    "    \n",
    "    def join_orders_with_dimensions(self, orders_df):\n",
    "        orders_df_with_latest_update = (\n",
    "            orders_df\n",
    "            .withColumn('rank', F.rank().over(Window.partitionBy(F.col('order_id'), F.col('status')).orderBy(F.col('event_timestamp').desc())))\n",
    "            .withColumn('event_timestap', F.min(F.col('event_timestamp')).over(Window.partitionBy(F.col('order_id'), F.col('status'))))\n",
    "            .where(F.col('rank') == 1)\n",
    "        )\n",
    "        joined_orders = (\n",
    "            orders_df_with_latest_update.alias('orders')\n",
    "            .join(self.dim_users.alias('dim_users'), F.col('orders.user_id') == F.col('dim_users.user_id'), 'left')\n",
    "            .join(self.dim_locations.alias('pickup_location'), F.col('orders.pickup_address') == F.col('pickup_location.location'), 'left')\n",
    "            .join(self.dim_locations.alias('delivery_location'), F.col('orders.delivery_address') == F.col('delivery_location.location'), 'left')\n",
    "            .select(\n",
    "                F.col('orders.order_id').cast(T.LongType())\n",
    "                , F.col('orders.package_description')\n",
    "                , F.col('orders.package_weight')\n",
    "                , F.col('orders.delivery_time')\n",
    "                , F.col('orders.created_at')\n",
    "                , F.col('orders.event_timestamp')\n",
    "                , F.col('orders.status')\n",
    "                , F.col('dim_users.user_key')\n",
    "                , F.col('pickup_location.location_key').alias('pick_up_location_key')\n",
    "                , F.col('delivery_location.location_key').alias('delivery_location_key')\n",
    "            )\n",
    "        )\n",
    "        return joined_orders\n",
    "\n",
    "    @staticmethod\n",
    "    def make_interval(start_date_key, start_time_key, end_date_key, end_time_key):\n",
    "\n",
    "        interval = (\n",
    "            F.to_timestamp(F.concat_ws(' ', end_date_key, end_time_key), 'yyyyMMdd HH:mm:ss') \n",
    "            - F.to_timestamp(F.concat_ws(' ', start_date_key, start_time_key), 'yyyyMMdd HH:mm:ss')\n",
    "        )\n",
    "\n",
    "        interval_struct = F.named_struct(\n",
    "            F.lit('days'), F.extract(F.lit('D'), interval)\n",
    "            , F.lit('hours'), F.extract(F.lit('H'), interval)\n",
    "            , F.lit('minutes'), F.extract(F.lit('m'), interval)\n",
    "            , F.lit('seconds'), F.extract(F.lit('s'), interval).cast(T.IntegerType())\n",
    "            \n",
    "        )\n",
    "        return interval_struct\n",
    "        \n",
    " \n",
    "    def process_fact_orders(self):\n",
    "        \n",
    "        enriched_orders_df = self.enriched_data_extractor.extract_enriched_orders()\n",
    "        joined_orders_df =  self.join_orders_with_dimensions(enriched_orders_df)\n",
    "\n",
    "        is_exists = DeltaTable.isDeltaTable(self.spark, self.fact_orders_path)\n",
    "        if not is_exists:\n",
    "            (\n",
    "                DeltaTable.create(self.spark)\n",
    "                .tableName(\"fact_orders\")\n",
    "                .addColumn(\"order_id\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"user_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"pick_up_location_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"delivery_location_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"dd_package_description\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"dd_status\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"package_weight\", \"FLOAT\", nullable = True)\n",
    "                .addColumn(\"created_order_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"created_order_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"accepted_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"accepted_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"in_transit_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"in_transit_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"delivered_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"delivered_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"delivery_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"delivery_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"created_to_accepted_lag\", \"STRUCT<days: INT, hours: INT, minutes: INT, seconds: INT>\", nullable = True)\n",
    "                .addColumn(\"accepted_to_in_transit_lag\", \"STRUCT<days: INT, hours: INT, minutes: INT, seconds: INT>\", nullable = True)\n",
    "                .addColumn(\"in_transit_to_delivered_lag\", \"STRUCT<days: INT, hours: INT, minutes: INT, seconds: INT>\", nullable = True)\n",
    "                .addColumn(\"delivered_and_delivery_difference\", \"STRUCT<days: INT, hours: INT, minutes: INT, seconds: INT>\", nullable = True)\n",
    "                .location(self.fact_orders_path)\n",
    "                .execute()\n",
    "            )\n",
    "   \n",
    "        processing_enriched_orders_df = joined_orders_df.where(F.col('status') == 'processing')    \n",
    "        accepted_enriched_orders_df = joined_orders_df.where(F.col('status') == 'accepted')  \n",
    "        in_transit_enriched_orders_df = joined_orders_df.where(F.col('status') == 'in_transit')  \n",
    "        delivered_enriched_orders_df = joined_orders_df.where(F.col('status') == 'delivered')  \n",
    "        fact_orders_df = self.spark.read.format('delta').option('path', self.fact_orders_path).load()\n",
    "      \n",
    "        source_orders_df = (\n",
    "            processing_enriched_orders_df.alias('p')\n",
    "            .join(accepted_enriched_orders_df.alias('a'), F.col('p.order_id') == F.col('a.order_id'), 'full')\n",
    "            .join(in_transit_enriched_orders_df.alias('it'), F.col('a.order_id') == F.col('it.order_id'), 'full')\n",
    "            .join(delivered_enriched_orders_df.alias('d'), F.col('it.order_id') == F.col('d.order_id'), 'full')\n",
    "            .join(fact_orders_df.alias('o'), F.col('d.order_id') == F.col('o.order_id'), 'left')\n",
    "        )\n",
    "        \n",
    "        final_source_orders_df = source_orders_df.select(\n",
    "            F.coalesce(F.col('d.order_id'), F.col('it.order_id'), F.col('a.order_id'), F.col('p.order_id')).alias('order_id')\n",
    "            , F.coalesce(F.col('d.user_key'), F.col('it.user_key'), F.col('a.user_key'), F.col('p.user_key')).alias('user_key')\n",
    "            , F.coalesce(F.col('d.pick_up_location_key'), F.col('it.pick_up_location_key'), F.col('a.pick_up_location_key'), F.col('p.pick_up_location_key')).alias('pick_up_location_key')\n",
    "            , F.coalesce(F.col('d.delivery_location_key'), F.col('it.delivery_location_key'), F.col('a.delivery_location_key'), F.col('p.delivery_location_key')).alias('delivery_location_key')\n",
    "            , F.coalesce(F.col('d.package_description'), F.col('it.package_description'), F.col('a.package_description'), F.col('p.package_description')).alias('dd_package_description')\n",
    "            , F.coalesce(F.col('d.status'), F.col('it.status'), F.col('a.status'), F.col('p.status')).alias('dd_status')\n",
    "            , F.coalesce(F.col('d.package_weight'), F.col('it.package_weight'), F.col('a.package_weight'), F.col('p.package_weight')).alias('package_weight')\n",
    "            , F.date_format(F.coalesce(F.col('d.created_at'), F.col('it.created_at'), F.col('a.created_at'), F.col('p.created_at')), 'yyyyMMdd').cast(T.IntegerType()).alias('created_order_date_key')\n",
    "            , F.date_format(F.coalesce(F.col('d.created_at'), F.col('it.created_at'), F.col('a.created_at'), F.col('p.created_at')), 'HH:mm:ss').alias('created_order_time_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(p.order_id) then cast(99991231 as int)\n",
    "                    when isnull(o.order_id) or isnull(p.order_id) then coalesce(cast(date_format(p.event_timestamp, 'yyyyMMdd') as int), o.accepted_date_key)\n",
    "                    when o.accepted_date_key == 99991231 then cast(date_format(p.event_timestamp, 'yyyyMMdd') as int)\n",
    "                    else o.accepted_date_key\n",
    "                end\n",
    "            ''').alias('accepted_date_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(p.order_id) then '00:00:00'\n",
    "                    when isnull(o.order_id) or isnull(p.order_id) then coalesce(date_format(p.event_timestamp, 'HH:mm:ss'), o.accepted_time_key)\n",
    "                    when o.accepted_time_key == '00:00:00' then date_format(p.event_timestamp, 'HH:mm:ss')\n",
    "                    else o.accepted_time_key\n",
    "                end\n",
    "            ''').alias('accepted_time_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(it.order_id) then cast(99991231 as int)\n",
    "                    when isnull(o.order_id) or isnull(it.order_id) then coalesce(cast(date_format(it.event_timestamp, 'yyyyMMdd') as int), o.in_transit_date_key)\n",
    "                    when o.in_transit_date_key == 99991231 then cast(date_format(it.event_timestamp, 'yyyyMMdd') as int)\n",
    "                    else o.in_transit_date_key\n",
    "                end\n",
    "            ''').alias('in_transit_date_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(it.order_id) then '00:00:00'\n",
    "                    when isnull(o.order_id) or isnull(it.order_id) then coalesce(date_format(it.event_timestamp, 'HH:mm:ss'), o.in_transit_time_key)\n",
    "                    when o.in_transit_time_key == '00:00:00' then date_format(it.event_timestamp, 'HH:mm:ss')\n",
    "                    else o.in_transit_time_key\n",
    "                end\n",
    "            ''').alias('in_transit_time_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(d.order_id) then cast(99991231 as int)\n",
    "                    when isnull(o.order_id) or isnull(d.order_id) then coalesce(cast(date_format(d.event_timestamp, 'yyyyMMdd') as int), o.delivered_date_key)\n",
    "                    when o.delivered_date_key == 99991231 then cast(date_format(d.event_timestamp, 'yyyyMMdd') as int)\n",
    "                    else o.delivered_date_key\n",
    "                end\n",
    "            ''').alias('delivered_date_key')\n",
    "            , F.expr('''\n",
    "                case \n",
    "                    when isnull(o.order_id) and isnull(d.order_id) then '00:00:00'\n",
    "                    when isnull(o.order_id) or isnull(d.order_id) then coalesce(date_format(d.event_timestamp, 'HH:mm:ss'), o.delivered_time_key)\n",
    "                    when o.delivered_time_key == '00:00:00' then date_format(d.event_timestamp, 'HH:mm:ss')\n",
    "                    else o.delivered_time_key\n",
    "                end\n",
    "            ''').alias('delivered_time_key')\n",
    "            , F.date_format(F.coalesce(F.col('d.delivery_time'), F.col('it.delivery_time'), F.col('a.delivery_time'), F.col('p.delivery_time')), 'yyyyMMdd').cast(T.IntegerType()).alias('delivery_date_key')\n",
    "            , F.date_format(F.coalesce(F.col('d.delivery_time'), F.col('it.delivery_time'), F.col('a.delivery_time'), F.col('p.delivery_time')), 'HH:mm:ss').alias('delivery_time_key')\n",
    "\n",
    "        )\n",
    "            \n",
    "        final_source_orders_df = final_source_orders_df.select(\n",
    "            F.col('*')\n",
    "            , (\n",
    "                F.when(\n",
    "                    F.col('accepted_date_key') != F.lit(99991231)\n",
    "                    , self.make_interval(F.col('created_order_date_key'), F.col('created_order_time_key'), F.col('accepted_date_key'), F.col('accepted_time_key'))\n",
    "                )\n",
    "                .otherwise(F.named_struct(F.lit('days'), F.lit(0), F.lit('hours'), F.lit(0), F.lit('minutes'), F.lit(0), F.lit('seconds'), F.lit(0)))\n",
    "            ).alias('created_to_accepted_lag')\n",
    "            , (\n",
    "                F.when(\n",
    "                    F.col('in_transit_date_key') != F.lit(99991231)\n",
    "                    , self.make_interval(F.col('accepted_date_key'), F.col('accepted_time_key'), F.col('in_transit_date_key'), F.col('in_transit_time_key'))\n",
    "                )\n",
    "                .otherwise(F.named_struct(F.lit('days'), F.lit(0), F.lit('hours'), F.lit(0), F.lit('minutes'), F.lit(0), F.lit('seconds'), F.lit(0)))\n",
    "            ).alias('accepted_to_in_transit_lag')\n",
    "            , (\n",
    "                F.when(\n",
    "                    F.col('in_transit_date_key') != F.lit(99991231)\n",
    "                    , self.make_interval(F.col('in_transit_date_key'), F.col('in_transit_time_key'), F.col('delivered_date_key'), F.col('delivery_time_key'))\n",
    "                )\n",
    "                .otherwise(F.named_struct(F.lit('days'), F.lit(0), F.lit('hours'), F.lit(0), F.lit('minutes'), F.lit(0), F.lit('seconds'), F.lit(0)))\n",
    "            ).alias('in_transit_to_delivered_lag')\n",
    "            , (\n",
    "                F.when(\n",
    "                    F.col('in_transit_date_key') != F.lit(99991231)\n",
    "                    , self.make_interval(F.col('delivered_date_key'), F.col('delivered_time_key'), F.col('delivery_date_key'), F.col('delivery_time_key'))\n",
    "                )\n",
    "                .otherwise(F.named_struct(F.lit('days'), F.lit(0), F.lit('hours'), F.lit(0), F.lit('minutes'), F.lit(0), F.lit('seconds'), F.lit(0)))\n",
    "            ).alias('delivered_and_delivery_difference')\n",
    "        )\n",
    "        \n",
    "        self.curated_data_loader.load_fact_orders(self, final_source_orders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b41c2-cd13-47da-b2d2-3bff7b1b7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fact_order = FactOrders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793744d-8e3c-4681-bb8b-f60522996941",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders_target = spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/facts/fact_orders').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4f325-8559-4f4e-b137-c14e8cfbe2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders_target.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786179e4-d603-41e8-81e3-605fba11fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = temp_fact_order.process_fact_orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33492d94-2ccd-428d-85f9-ca717acb0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.orderBy(F.col('order_id')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402502e-b2de-4cd7-992c-bad0b593d267",
   "metadata": {},
   "source": [
    "# fact_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7b46d-60a0-42f8-80d2-a073621718ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae64f7-0f9e-427d-8a4a-89080391bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactPayments(Table):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fact_payments_path = self.fact_base_path + '/fact_payments'\n",
    "        self.fact_orders_path = self.fact_base_path + '/fact_orders'\n",
    "        \n",
    "    @property\n",
    "    def fact_orders(self):\n",
    "        fact_orders = self.spark.read.format('delta').option('path', self.fact_orders_path).load()\n",
    "        return fact_orders\n",
    "\n",
    "    def process_fact_payments(self):\n",
    "        enriched_payments_df = self.enriched_data_extractor.extract_enriched_payments()\n",
    "\n",
    "        is_exists = DeltaTable.isDeltaTable(self.spark, self.fact_payments_path)\n",
    "        if not is_exists:\n",
    "            (\n",
    "                DeltaTable.create(spark)\n",
    "                .tableName(\"payments\")\n",
    "                .addColumn(\"payment_id\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"order_id\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"user_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"pick_up_location_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"delivery_location_key\", \"LONG\", nullable = True)\n",
    "                .addColumn(\"dd_package_description\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"created_order_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"created_order_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"payment_date_key\", \"INT\", nullable = True)\n",
    "                .addColumn(\"payment_time_key\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"dd_payment_method\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"dd_payment_status\", \"STRING\", nullable = True)\n",
    "                .addColumn(\"package_weight\", \"FLOAT\", nullable = True)\n",
    "                .addColumn(\"amount\", \"DECIMAL(10,2)\", nullable = True)\n",
    "                .location(self.fact_payments_path)\n",
    "                .execute()\n",
    "            )\n",
    "        \n",
    "        joined_payments_df = (\n",
    "            enriched_payments_df.alias('p')\n",
    "            .join(self.fact_orders.alias('o'), F.col('o.order_id') == F.col('p.order_id'), 'left' )\n",
    "            .select(\n",
    "                F.col('p.payment_id').cast(T.LongType()).alias('payment_id')\n",
    "                , F.col('p.order_id').cast(T.LongType()).alias('order_id')\n",
    "                , F.col('o.user_key').cast(T.LongType()).alias('user_key')\n",
    "                , F.col('o.pick_up_location_key').cast(T.LongType()).alias('pick_up_location_key')\n",
    "                , F.col('o.delivery_location_key').cast(T.LongType()).alias('delivery_location_key')\n",
    "                , F.col('o.dd_package_description').cast(T.LongType()).alias('dd_package_description')\n",
    "                , F.col('o.created_order_date_key').alias('created_order_date_key')\n",
    "                , F.col('o.created_order_time_key').alias('created_order_time_key')\n",
    "                , F.date_format(F.col('payment_date'), 'yyyyMMdd').cast(T.IntegerType()).alias('payment_date_key')\n",
    "                , F.date_format(F.col('payment_date'), 'HH:mm:ss').alias('payment_time_key')\n",
    "                , F.col('payment_method').alias('dd_payment_method')\n",
    "                , F.col('payment_status').alias('dd_payment_status')\n",
    "                , F.col('o.package_weight')\n",
    "                , F.col('p.amount')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.curated_data_loader.load_fact_payments(self, joined_payments_df)\n",
    "\n",
    "temp = FactPayments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306880f-b29f-4968-bb4c-4e3e5a79754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.process_fact_payments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4713fc-aafc-4b92-922c-c5dfb3cf6727",
   "metadata": {},
   "source": [
    "# fact_shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe63d6-638a-44d4-b165-41564cbe3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/facts/fact_orders').load().where(F.col('dd_package_description').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a83d2-6dd1-4e18-bf0f-b0a18b2e0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "14 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe456cb8-4857-44b6-911f-50d2806ea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_shipments = spark.read.format('parquet').option('path', '/enriched/transactional/mysql/logistics/shipments').load()\n",
    "dim_drivers = spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/dimensions/dim_drivers').load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a35a92-0a04-4145-b003-32040bcbf34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------------------+------------+------------+\n",
      "|driver_id|user_id|vehicle_license_plate|vehicle_type|vehicle_year|\n",
      "+---------+-------+---------------------+------------+------------+\n",
      "|1        |5      |XYZ-7598             |car         |2025        |\n",
      "|2        |6      |XYZ-8290             |car         |2025        |\n",
      "|3        |7      |XYZ-6799             |bike        |2025        |\n",
      "|4        |11     |XYZ-6274             |car         |2025        |\n",
      "|5        |14     |XYZ-4392             |truck       |2025        |\n",
      "|6        |16     |XYZ-9649             |car         |2025        |\n",
      "|7        |17     |XYZ-2844             |truck       |2025        |\n",
      "|8        |20     |XYZ-1473             |bike        |2025        |\n",
      "|9        |22     |XYZ-5097             |bike        |2025        |\n",
      "|10       |24     |XYZ-1781             |truck       |2025        |\n",
      "|11       |25     |XYZ-4491             |truck       |2025        |\n",
      "|12       |26     |XYZ-6596             |car         |2025        |\n",
      "|13       |27     |XYZ-3209             |bike        |2025        |\n",
      "|14       |29     |XYZ-7567             |truck       |2025        |\n",
      "|15       |30     |XYZ-1004             |car         |2025        |\n",
      "|16       |32     |XYZ-5947             |bike        |2025        |\n",
      "|17       |35     |XYZ-3893             |bike        |2025        |\n",
      "|18       |36     |XYZ-4571             |car         |2025        |\n",
      "|19       |37     |XYZ-6437             |car         |2025        |\n",
      "|20       |38     |XYZ-7316             |truck       |2025        |\n",
      "|61       |131    |XYZ-2798             |bike        |2025        |\n",
      "|62       |132    |XYZ-4787             |bike        |2025        |\n",
      "|63       |137    |XYZ-1412             |bike        |2025        |\n",
      "|64       |138    |XYZ-8519             |bike        |2025        |\n",
      "|65       |141    |XYZ-6486             |bike        |2025        |\n",
      "|66       |142    |XYZ-9995             |truck       |2025        |\n",
      "|67       |143    |XYZ-7132             |truck       |2025        |\n",
      "|68       |144    |XYZ-3171             |bike        |2025        |\n",
      "|69       |145    |XYZ-2920             |truck       |2025        |\n",
      "|70       |146    |XYZ-4297             |car         |2025        |\n",
      "|71       |147    |XYZ-3101             |bike        |2025        |\n",
      "|72       |148    |XYZ-4876             |car         |2025        |\n",
      "|73       |152    |XYZ-3043             |car         |2025        |\n",
      "|74       |154    |XYZ-1461             |car         |2025        |\n",
      "|75       |155    |XYZ-2981             |truck       |2025        |\n",
      "|76       |160    |XYZ-1181             |bike        |2025        |\n",
      "|77       |161    |XYZ-1144             |bike        |2025        |\n",
      "|78       |162    |XYZ-8438             |truck       |2025        |\n",
      "|79       |164    |XYZ-2624             |car         |2025        |\n",
      "|80       |165    |XYZ-2103             |bike        |2025        |\n",
      "|21       |39     |XYZ-8224             |truck       |2025        |\n",
      "|22       |42     |XYZ-6216             |truck       |2025        |\n",
      "|23       |45     |XYZ-3775             |car         |2025        |\n",
      "|24       |50     |XYZ-4452             |car         |2025        |\n",
      "|25       |52     |XYZ-8343             |truck       |2025        |\n",
      "|26       |53     |XYZ-3490             |truck       |2025        |\n",
      "|27       |55     |XYZ-3779             |bike        |2025        |\n",
      "|28       |56     |XYZ-5561             |truck       |2025        |\n",
      "|29       |59     |XYZ-6555             |car         |2025        |\n",
      "|30       |60     |XYZ-9223             |truck       |2025        |\n",
      "|31       |61     |XYZ-4971             |truck       |2025        |\n",
      "|32       |62     |XYZ-5678             |truck       |2025        |\n",
      "|33       |63     |XYZ-2706             |bike        |2025        |\n",
      "|34       |64     |XYZ-7861             |car         |2025        |\n",
      "|35       |66     |XYZ-2027             |truck       |2025        |\n",
      "|36       |69     |XYZ-5003             |truck       |2025        |\n",
      "|37       |70     |XYZ-3328             |bike        |2025        |\n",
      "|38       |74     |XYZ-8969             |bike        |2025        |\n",
      "|39       |77     |XYZ-5161             |car         |2025        |\n",
      "|40       |78     |XYZ-9576             |bike        |2025        |\n",
      "|41       |80     |XYZ-9586             |bike        |2025        |\n",
      "|42       |81     |XYZ-5607             |car         |2025        |\n",
      "|43       |88     |XYZ-5485             |bike        |2025        |\n",
      "|44       |91     |XYZ-4121             |bike        |2025        |\n",
      "|45       |93     |XYZ-9839             |car         |2025        |\n",
      "|46       |94     |XYZ-2447             |bike        |2025        |\n",
      "|47       |95     |XYZ-7886             |bike        |2025        |\n",
      "|48       |96     |XYZ-1931             |car         |2025        |\n",
      "|49       |99     |XYZ-6686             |bike        |2025        |\n",
      "|50       |100    |XYZ-2954             |bike        |2025        |\n",
      "|51       |101    |XYZ-8752             |bike        |2025        |\n",
      "|52       |103    |XYZ-9695             |bike        |2025        |\n",
      "|53       |110    |XYZ-9204             |truck       |2025        |\n",
      "|54       |112    |XYZ-4141             |truck       |2025        |\n",
      "|55       |113    |XYZ-5862             |bike        |2025        |\n",
      "|56       |116    |XYZ-4330             |bike        |2025        |\n",
      "|57       |118    |XYZ-4519             |truck       |2025        |\n",
      "|58       |120    |XYZ-3052             |truck       |2025        |\n",
      "|59       |124    |XYZ-1214             |bike        |2025        |\n",
      "|60       |126    |XYZ-1344             |car         |2025        |\n",
      "+---------+-------+---------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format('avro')\n",
    "    .option('path', '/raw/transactional/mysql/logistics/topics/logistics_src.logistics.Drivers')\n",
    "    .load()\n",
    "    .where(F.col('after.*'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6380a97-11b8-4bed-b842-1f6c8b9b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_shipments.alias('s').join(dim_drivers.alias('d'), F.col('s.driver_id') == F.col('d.driver_id') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705bc64-6177-4535-944b-232fea5a6116",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfc681-812e-452b-aac2-c415faeac411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0b52c-2868-4f87-af65-5f7916ee15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop table fact_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93251bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0ff2d4-7581-49c8-a675-1ac04c1ef1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4084dd4-9e7b-43e1-9e94-4b215ec24183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_fact_payments, process_fact_shipments, process_fact_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3672923-f621-46a2-a611-cda4df7d23c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 08:10:12,305 - logistics - INFO - HDFS Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    }
   ],
   "source": [
    "process_fact_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6ee53-5d38-44ca-bcfb-59780965605b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4779d-b14d-4d38-9a95-76185136e606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
