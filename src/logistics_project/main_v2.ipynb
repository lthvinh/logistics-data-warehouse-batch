{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2d7b499b-5ae6-47f8-8d0e-8571c5998e01",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from extract import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "47368992-15ff-4f75-b9e3-14f1819d56c8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "test.temp.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "feb4d677-4215-47aa-8ed2-df14cd400f9e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "import pendulum\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'vinh'\n",
    "    , 'email': 'ltvinh1101@gmail.com'\n",
    "    , 'email_on_failure': True\n",
    "    , 'email_on_retry': True\n",
    "    , 'retries': 2\n",
    "    , 'retry_delay': pendulum.duration(seconds = 1)\n",
    "}\n",
    "    \n",
    "with DAG(\n",
    "    dag_id = 'logistics_dag_v1'\n",
    "    , description = 'This is a dag'\n",
    "    , start_date = pendulum.now(tz = 'Asia/Ho_Chi_Minh')\n",
    "    , default_args = default_args\n",
    "    , catchup = False\n",
    "    , schedule = '@daily'\n",
    "    , tags = ['logistics', 'mysql']\n",
    ") as dag:\n",
    "    #START\n",
    "    start = EmptyOperator(task_id = 'start')\n",
    "\n",
    "    config_path = Path(__file__) / 'config/config.yaml'\n",
    "    with open(config_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        packages = config['spark']['packages']\n",
    "        \n",
    "    process_enirched_apps = [\n",
    "        'process_enriched_drivers'\n",
    "        # , 'process_enriched_drivers'\n",
    "        # , 'process_enriched_orders'\n",
    "        # , 'process_enriched_paymentss'\n",
    "        # , 'process_enriched_shipments'\n",
    "    ]\n",
    "\n",
    "    processe_enriched_operators = []\n",
    "    for app_name in process_enirched_apps:\n",
    "        temp = SparkSubmitOperator(\n",
    "            task_id = app_name\n",
    "            , application = Path(__file__) / f'workflows/{app_name}.py'\n",
    "            , packages = ','.join(packages)\n",
    "        )\n",
    "\n",
    "    #END\n",
    "    end = EmptyOperator(task_id = 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ee08764e-1b80-4025-8b52-8f9b043fb611",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from workflows import process_enriched_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "91a49fe8-54e5-4ec1-ab66-1ad71e9de9ad",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from airflow.operators.empty import EmptyOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2acd185a-4e32-4222-a660-e889c30c3837",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "process_enriched_users.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7f479c70-1711-461e-9f8a-251de5d04eea",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from workflows import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d56e40d1-e338-420e-9738-35bb8c569c55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "31492fe5-b3e7-46f5-ab63-ebe3de0013d4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "    config_path = Path.cwd() / 'config/config.yaml'\n",
    "    with open(config_path) as file:\n",
    "        config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f7d7e876-28f2-493b-b6b8-a90413eaf265",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "config['spark']['packages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "aab5b845-52ca-4fcf-b4c8-c09ff46ecaa2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "process_enriched_drivers.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5b54ce38-ba07-4f90-9d27-74d72f49dc91",
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ac4e86d3-c2b2-4a80-9c52-192b06d58117",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "daebb23a-1866-4ddc-9a4a-f1e2f4df2d49",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "Path() / '123/123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9ee9bf4-649e-409b-bb33-ecdbcd3d8b6b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dfcd5e04-4825-4ef6-bb70-508e6e8c6671",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a47ef722-d75a-4a97-bb8e-218b805a5dc1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark.read.format('parquet').option('path', 'hdfs://hdfs-namenode:9000/enriched/transactional/mysql/logistics/users').load().orderBy(F.col('user_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4fbcb05a-c64a-464b-9007-95d022975e04",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "60bbee67-887b-4c4e-93f8-593507d24207",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from workflows import process_enriched_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "37bbcb77-f789-4197-b4a4-4bf6a2b45619",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "process_enriched_users.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "883ac07c-4f36-4501-8ebf-921efb64bfc1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2f1c30d4-38b2-4648-8a8f-7a5e95af3d8d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "56cc310d-c93a-4d50-bceb-3219ff146009",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark.read.option('path', '/enriched/transactional/mysql/logistics/users/year=2025/month=2/day=3').load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c082b01b-6c3c-40b1-9249-6e296050e16d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.models.baseoperator import chain\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from pathlib import Path\n",
    "import pendulum\n",
    "\n",
    "enriched_path = Path('/opt/spark-apps/logistics_project_v2/workflows/enriched')\n",
    "curated_path = Path('/opt/spark-apps/logistics_project_v2/workflows/curated')\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'vinh'\n",
    "    , 'email': 'ltvinh1101@gmail.com'\n",
    "    , 'email_on_failure': True\n",
    "    , 'email_on_retry': True\n",
    "    , 'retries': 2\n",
    "    , 'retry_delay': pendulum.duration(seconds = 1)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id = 'logistics_dag_v1'\n",
    "    , description = 'this is my first dag'\n",
    "    , start_date = pendulum.now(tz = 'Asia/Ho_Chi_Minh')\n",
    "    # , schedule = '@daily'\n",
    "    , schedule = None\n",
    "    , tags = ['logistics', 'mysql']\n",
    ") as dag:\n",
    "    # START\n",
    "    start = EmptyOperator(task_id = 'start')\n",
    "\n",
    "    \n",
    "    # ENRICHED\n",
    "\n",
    "    with TaskGroup(group_id = 'enriched_operators_group') as enriched_group:\n",
    "        process_enirched_apps = [\n",
    "            'process_enriched_users'\n",
    "            , 'process_enriched_drivers'\n",
    "            , 'process_enriched_orders'\n",
    "            , 'process_enriched_paymentss'\n",
    "            , 'process_enriched_shipments'\n",
    "        ]\n",
    "        for app_name in process_enirched_apps:\n",
    "            SparkSubmitOperator(\n",
    "                task_id = app_name\n",
    "                , conn_id = 'spark_conn'\n",
    "                , application = str(enriched_path / f'{app_name}.py')\n",
    "                , packages = 'io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.5.3'\n",
    "            )\n",
    "     \n",
    "    start >> enriched_group\n",
    "\n",
    "    #CURATED\n",
    "    \n",
    "    with TaskGroup(group_id = 'dim_operatprs_group') as dim_group:\n",
    "        process_dim_apps = [\n",
    "            'process_dim_users'\n",
    "            , 'process_dim_drivers'\n",
    "            , 'process_dim_locations'\n",
    "            , 'process_dim_date'\n",
    "\n",
    "        ]\n",
    "        \n",
    "        for app_name in process_dim_apps:\n",
    "            SparkSubmitOperator(\n",
    "                task_id = app_name\n",
    "                , conn_id = 'spark_conn'\n",
    "                , application = str(curated_path / f'{app_name}.py')\n",
    "                , packages = 'io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.5.3'\n",
    "            )\n",
    "            \n",
    "    with TaskGroup(group_id = 'fact_operators_groups') as fact_group:\n",
    "        process_fact_apps = [\n",
    "            'process_fact_processing_orders'\n",
    "            , 'process_fact_in_transit_orders'\n",
    "            , 'process_fact_accepted_orders'\n",
    "            , 'process_fact_delivered_orders'\n",
    "        ]\n",
    "        \n",
    "        for app_name in process_fact_apps:\n",
    "            SparkSubmitOperator(\n",
    "                task_id = app_name\n",
    "                , conn_id = 'spark_conn'\n",
    "                , application = str(curated_path / f'{app_name}.py')\n",
    "                , packages = 'io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.5.3'\n",
    "            )\n",
    "\n",
    "    enriched_group >>  [dim_group, fact_group]\n",
    "\n",
    "    \n",
    "    # END\n",
    "    \n",
    "    end = EmptyOperator(task_id = 'end')\n",
    "\n",
    "    [dim_group, fact_group] >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.utils.task_group import TaskGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models.baseoperator import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master spark://spark-master:7077 /home/jovyan/work/src/spark_dataframe_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.enriched import process_enriched_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_users.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --packages io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.5.3 workflows/enriched/process_enriched_users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.enriched import process_enriched_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.enriched import process_enriched_shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_enriched_shipments.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dim_date.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_dim_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 06:53:32,405 - logistics - INFO - HDFS Path: /enriched/transactional/mysql/logistics/users\n"
     ]
    }
   ],
   "source": [
    "process_dim_users.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/dimensions/dim_users').load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_dim_drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_dim_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_fact_processing_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 07:11:45,507 - logistics - INFO - HDFS Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    }
   ],
   "source": [
    "process_fact_processing_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/dimensions/dim_drivers').load().orderBy(F.col('driver_id')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/dimensions/dim_locations').load().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"SimpleDataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/dimensions/dim_users').load().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').option('path', '/curated/transactional/mysql/logistics/facts/fact_orders').load().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.curated import process_fact_delivered_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 07:17:55,476 - logistics - INFO - HDFS Path: /enriched/transactional/mysql/logistics/orders\n"
     ]
    }
   ],
   "source": [
    "process_fact_delivered_orders.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://livy:8998/batches'\n",
    "\n",
    "data = {\n",
    "    'file': '/home/jovyan/work/src/logistics_project_v2/spark_dataframe_app.py'\n",
    "}\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
